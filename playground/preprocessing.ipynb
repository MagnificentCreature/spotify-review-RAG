{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# from llama_index import (Document)\n",
    "\n",
    "# Load documents from the review dataset directory (adjust path as needed)\n",
    "csv_path = \"../Data/SPOTIFY_REVIEWS_DEDUP.csv\"\n",
    "df = pd.read_csv(csv_path, usecols=[\"review_text\",\"review_rating\",\"review_likes\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# from llama_index import (Document)\n",
    "\n",
    "# Load documents from the review dataset directory (adjust path as needed)\n",
    "csv_path = \"../Data/SPOTIFY_REVIEWS.csv\"\n",
    "df = pd.read_csv(csv_path, usecols=[\"review_text\",\"review_rating\",\"review_likes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m remove_short_lines(df)\n\u001b[1;32m---> 17\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[1;32mIn[27], line 10\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      6\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_text\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_text\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      9\u001b[0m     \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreview_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m---> 10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mW+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# replace non-alphanumeric with space\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# collapse whitespace\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:137\u001b[0m, in \u001b[0;36mforbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use .str.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with values of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred dtype \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:1567\u001b[0m, in \u001b[0;36mStringMethods.replace\u001b[1;34m(self, pat, repl, n, case, flags, regex)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m case \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     case \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1567\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str_replace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregex\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_result(result)\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\strings\\object_array.py:179\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_replace\u001b[1;34m(self, pat, repl, n, case, flags, regex)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mreplace(pat, repl, n)\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\strings\\object_array.py:78\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_map\u001b[1;34m(self, f, na_value, dtype, convert)\u001b[0m\n\u001b[0;32m     76\u001b[0m map_convert \u001b[38;5;241m=\u001b[39m convert \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(mask)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_convert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# Reraise the exception if callable `f` got wrong number of args.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# The user may want to be warned by this, instead of getting NaN\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     p_err \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m((takes)|(missing)) (?(2)from \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+ to )?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(?(3)required )positional arguments?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2895\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer_mask\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mlib.pyx:2932\u001b[0m, in \u001b[0;36mpandas._libs.lib._map_infer_mask\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\strings\\object_array.py:175\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_replace.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    172\u001b[0m         pat \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(pat, flags\u001b[38;5;241m=\u001b[39mflags)\n\u001b[0;32m    174\u001b[0m     n \u001b[38;5;241m=\u001b[39m n \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 175\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: pat\u001b[38;5;241m.\u001b[39msub(repl\u001b[38;5;241m=\u001b[39mrepl, string\u001b[38;5;241m=\u001b[39mx, count\u001b[38;5;241m=\u001b[39mn)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mreplace(pat, repl, n)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# remove rows with review text less than 5 tokens\n",
    "def remove_short_lines(df, length=4):\n",
    "    return df.loc[df[\"review_text\"].str.count(\" \") >= length]\n",
    "\n",
    "def preprocess_text(df):\n",
    "    df.drop_duplicates(subset=[\"review_text\"], inplace=True)\n",
    "    df.dropna(subset=[\"review_text\"], inplace=True)\n",
    "    df[\"review_text\"] = (\n",
    "        df[\"review_text\"]\n",
    "        .str.replace(r'\\W+', ' ', regex=True)  # replace non-alphanumeric with space\n",
    "        .str.replace(r'\\s+', ' ', regex=True)  # collapse whitespace\n",
    "        .str.lower()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df = remove_short_lines(df)\n",
    "df = preprocess_text(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Apply to DataFrame\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect_spelling_parallel_joblib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# df[\"review_text\"] = df[\"review_text\"].apply(lambda x: correct_spelling(x))\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# df[\"review_text\"] = correct_spelling_parallel(df)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 16\u001b[0m, in \u001b[0;36mcorrect_spelling_parallel_joblib\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcorrect_spelling_parallel_joblib\u001b[39m(df):\n\u001b[1;32m---> 16\u001b[0m     corrected_reviews \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrect_spelling\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m corrected_reviews\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1703\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1703\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_abort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1706\u001b[0m     \u001b[38;5;66;03m# Store the unconsumed tasks and terminate the workers if necessary\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1614\u001b[0m, in \u001b[0;36mParallel._abort\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborted \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabort_everything\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m   1610\u001b[0m     \u001b[38;5;66;03m# If the backend is managed externally we need to make sure\u001b[39;00m\n\u001b[0;32m   1611\u001b[0m     \u001b[38;5;66;03m# to leave it in a working state to allow for future jobs\u001b[39;00m\n\u001b[0;32m   1612\u001b[0m     \u001b[38;5;66;03m# scheduling.\u001b[39;00m\n\u001b[0;32m   1613\u001b[0m     ensure_ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_managed_backend\n\u001b[1;32m-> 1614\u001b[0m     \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabort_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensure_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ready\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\_parallel_backends.py:620\u001b[0m, in \u001b[0;36mLokyBackend.abort_everything\u001b[1;34m(self, ensure_ready)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mabort_everything\u001b[39m(\u001b[38;5;28mself\u001b[39m, ensure_ready\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    618\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Shutdown the workers and restart a new one with the same parameters\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ensure_ready:\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\executor.py:75\u001b[0m, in \u001b[0;36mMemmappingExecutor.terminate\u001b[1;34m(self, kill_workers)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mterminate\u001b[39m(\u001b[38;5;28mself\u001b[39m, kill_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkill_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# When workers are killed in a brutal manner, they cannot execute the\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# finalizer of their shared memmaps. The refcount of those memmaps may\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# be off by an unknown number, so instead of decref'ing them, we force\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# with allow_non_empty=True but if we can't, it will be clean up later\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# on by the resource_tracker.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submit_resize_lock:\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:1303\u001b[0m, in \u001b[0;36mProcessPoolExecutor.shutdown\u001b[1;34m(self, wait, kill_workers)\u001b[0m\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executor_manager_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m wait:\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;66;03m# This locks avoids concurrent join if the interpreter\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m     \u001b[38;5;66;03m# is shutting down.\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _global_shutdown_lock:\n\u001b[1;32m-> 1303\u001b[0m         \u001b[43mexecutor_manager_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1304\u001b[0m         _threads_wakeups\u001b[38;5;241m.\u001b[39mpop(executor_manager_thread, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# To reduce the risk of opening too many files, remove references to\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;66;03m# objects that use file descriptors.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:1149\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1149\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:1169\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1170\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1171\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Load SymSpell dictionary (ensure you have \"frequency_dictionary_en.txt\")\n",
    "dictionary_path = importlib.resources.files(\"symspellpy\") / \"frequency_dictionary_en_82_765.txt\"\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2)\n",
    "sym_spell.load_dictionary(str(dictionary_path), term_index=0, count_index=1)\n",
    "\n",
    "def correct_spelling(text):\n",
    "    words = text.split()  # Split the sentence into words\n",
    "    corrected_words = [sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)[0].term if sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2) else word for word in words]\n",
    "    return ' '.join(corrected_words)  # Reassemble the sentence\n",
    "\n",
    "def correct_spelling_parallel_joblib(df):\n",
    "    corrected_reviews = Parallel(n_jobs=-1)(delayed(correct_spelling)(text) for text in df['review_text'])\n",
    "    df['review_text'] = corrected_reviews\n",
    "    return df\n",
    "\n",
    "# Apply to DataFrame\n",
    "df = correct_spelling_parallel_joblib(df)\n",
    "\n",
    "\n",
    "# df[\"review_text\"] = df[\"review_text\"].apply(lambda x: correct_spelling(x))\n",
    "# df[\"review_text\"] = correct_spelling_parallel(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\seanw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\seanw\\AppData\\Local\\Temp\\ipykernel_9012\\4175238052.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review_text'] = df['review_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(df):\n",
    "    df['review_text'] = df['review_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "remove_stopwords(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1423506, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = remove_short_lines(df, length=4)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enjoy awesome ui app music one ask</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>awesome ui best music app</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>professional android developer glad see spotif...</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>put amount much would spent without spotify sa...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>easy search discover new music also fast enoug...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>blir bara bättre och bättre</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>updating latest version got download offline p...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>music service use keep updates coming</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cant really fault exactly says</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>great way listen ur favourate music eould reco...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>works really well impressed keep guys</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>nice job spotify rather update existing app pu...</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>best app ever n nvr 2 deal pandora ever music ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>great source music surprising titles</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>app awesome get new songs like 3 seconds tje s...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          review_text  review_rating  \\\n",
       "1                  enjoy awesome ui app music one ask              5   \n",
       "3                           awesome ui best music app              5   \n",
       "4   professional android developer glad see spotif...              5   \n",
       "5   put amount much would spent without spotify sa...              5   \n",
       "8   easy search discover new music also fast enoug...              5   \n",
       "10                        blir bara bättre och bättre              4   \n",
       "12  updating latest version got download offline p...              1   \n",
       "14              music service use keep updates coming              5   \n",
       "15                     cant really fault exactly says              5   \n",
       "19  great way listen ur favourate music eould reco...              4   \n",
       "20              works really well impressed keep guys              5   \n",
       "23  nice job spotify rather update existing app pu...              1   \n",
       "25  best app ever n nvr 2 deal pandora ever music ...              5   \n",
       "26               great source music surprising titles              5   \n",
       "27  app awesome get new songs like 3 seconds tje s...              5   \n",
       "\n",
       "    review_likes  \n",
       "1              4  \n",
       "3              1  \n",
       "4             10  \n",
       "5              4  \n",
       "8              2  \n",
       "10             0  \n",
       "12             4  \n",
       "14             0  \n",
       "15             0  \n",
       "19             0  \n",
       "20             0  \n",
       "23            38  \n",
       "25             0  \n",
       "26             0  \n",
       "27             3  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as cuTfidfVectorizer\n",
    "from cuml.metrics.pairwise_distances import pairwise_distances\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import cupy as cp  # GPU-accelerated NumPy\n",
    "import cupyx.scipy.sparse as cpx  # Sparse CuPy operations\n",
    "from cuml.neighbors import NearestNeighbors  # RAPIDS GPU-optimized ANN\n",
    "\n",
    "def vectorized_deduplicate_dataframe(df, text_column=\"review_text\", threshold=0.8):\n",
    "    \"\"\"\n",
    "    Efficiently deduplicates a dataframe based on text similarity using TF-IDF and cosine similarity on GPU using RAPIDS cuML.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): Input DataFrame containing text data.\n",
    "    - text_column (str): Column name of text data.\n",
    "    - threshold (float): Similarity threshold (0 to 1), where higher means stricter deduplication.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Deduplicated DataFrame with all original columns preserved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute TF-IDF embeddings on GPU\n",
    "    vectorizer = cuTfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[text_column])  # Sparse matrix\n",
    "\n",
    "    print(\"Fitted and normalised matrix\")\n",
    "\n",
    "    # tfidf_matrix_gpu = cp.asarray(tfidf_matrix.toarray())  # Move data to GPU\n",
    "\n",
    "    # # Compute cosine similarity matrix on GPU\n",
    "    # similarity_matrix_gpu = pairwise_distances(tfidf_matrix_gpu, tfidf_matrix_gpu, metric=\"cosine\")\n",
    "    # print(\"Computed similarity matrix on GPU\")\n",
    "\n",
    "    # Convert to sparse CuPy matrix (Keep it sparse!)\n",
    "    tfidf_matrix_gpu = cpx.csr_matrix(tfidf_matrix)  \n",
    "\n",
    "    # Use RAPIDS NearestNeighbors (FAISS alternative)\n",
    "    nn = NearestNeighbors(n_neighbors=5, metric=\"cosine\", algorithm=\"brute\", output_type=\"numpy\")\n",
    "    nn.fit(tfidf_matrix_gpu)\n",
    "\n",
    "    print(\"Computed Nearest Neighbors on GPU\")\n",
    "\n",
    "    # Identify duplicates\n",
    "    unique_indexes = []\n",
    "    seen = set()\n",
    "\n",
    "    distances, indices = nn.kneighbors(tfidf_matrix_gpu, n_neighbors=5)\n",
    "\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        if i in seen:\n",
    "            continue\n",
    "        # Find similar reviews\n",
    "        # similar_indexes = pairwise_distances(similarity_matrix_gpu[i], similarity_matrix_gpu[i], metric=\"cosine\").toarray()\n",
    "        # similar_indexes = [idx for idx, val in enumerate(similar_indexes) if val > threshold]\n",
    "\n",
    "        similar_indexes = indices[i][distances[i] < threshold]        \n",
    "        seen.update(similar_indexes)  # Mark them as seen\n",
    "        unique_indexes.append(i)  # Keep only the first occurrence\n",
    "\n",
    "    # Return deduplicated DataFrame\n",
    "    return df.iloc[unique_indexes].reset_index(drop=True)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df_dedup1 = vectorized_deduplicate_dataframe(df)\n",
    "\n",
    "print(df_dedup1.shape)\n",
    "print(df.shape)\n",
    "\n",
    "df = df_dedup1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "data = {\n",
    "    \"review_text\": [\n",
    "        \"This product is amazing!\",  # Original\n",
    "        \"This product is really amazing!\",  # Slight variation\n",
    "        \"Worst product ever.\",  # Different\n",
    "        \"Absolutely terrible, do not buy.\",  # Different\n",
    "        \"Amazing product! Really good.\",  # Similar to first\n",
    "        \"I love this item, it's fantastic!\",  # Different\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_test = pd.DataFrame(data)\n",
    "\n",
    "# Run the deduplication function\n",
    "df_dedup = vectorized_deduplicate_dataframe(df_test, threshold=0.8)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nOriginal DataFrame:\")\n",
    "print(df_test)\n",
    "\n",
    "print(\"\\nDeduplicated DataFrame:\")\n",
    "print(df_dedup)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "df = df.dropna(subset=[\"review_text\"])  # or df[\"review_text\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# save this as a new csv\n",
    "df.to_csv(\"../Data/SPOTIFY_REVIEWS_DEDUP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#show 30 lines with length 5\n",
    "df[df[\"review_text\"].str.count(\" \") == 4].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from textblob import TextBlob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('punkt')\n",
    "\n",
    "df['num_tokens'] = df['review_text'].apply(lambda text: len(TextBlob(text).words))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['num_tokens'], kde=True, bins=50, color='blue')\n",
    "plt.xlabel('Number of tokens per review')\n",
    "plt.ylabel('Frequency')\n",
    "df.drop(columns=['num_tokens'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the reviews are quite short, we do not need to split them using text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# module_name = \"faiss\"\n",
    "# if module_name in sys.modules:\n",
    "#     del sys.modules[module_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import faiss\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "models = {\n",
    "    'all-MiniLM-L6-v2': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    'multi-qa-MiniLM-L6-dot-v1': 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1',\n",
    "    'paraphrase-MiniLM-L6-v2': 'sentence-transformers/paraphrase-MiniLM-L6-v2',\n",
    "}\n",
    "\n",
    "def select_model(m_key):\n",
    "    if m_key not in models:\n",
    "        raise ValueError(f'Model key is not found')\n",
    "    return SentenceTransformer(m_key, device=device)\n",
    "\n",
    "# import faiss\n",
    "# import torch\n",
    "\n",
    "# # Assume you have an index (e.g. a flat index on CPU)\n",
    "# cpu_index = faiss.IndexFlatL2(128)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"Using FAISS GPU\")\n",
    "#     # Create GPU resources and transfer the index to GPU 0\n",
    "#     res = faiss.StandardGpuResources()\n",
    "#     gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "#     index = gpu_index\n",
    "# else:\n",
    "#     print(\"Using FAISS CPU\")\n",
    "#     index = cpu_index\n",
    "\n",
    "# # Now use `index` for your similarity search\n",
    "\n",
    "# def encode_in_batches(texts, embedding_model, batch_size=512):\n",
    "#     sample_emb = embedding_model.encode([df.iloc[0]['review_text']], device=device)\n",
    "#     emb_dim = len(sample_emb[0])\n",
    "\n",
    "#     index = faiss.IndexFlatL2(emb_dim)\n",
    "\n",
    "#     for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding batches\"):\n",
    "#         batch_texts = texts[i:i + batch_size]\n",
    "#         emb = embedding_model.encode(batch_texts, device=device)\n",
    "#         index.add(emb)\n",
    "\n",
    "#     print(\"Compressing index\")\n",
    "#     m = 64  # Number of sub-vectors\n",
    "#     clusters = 100  # Number of clusters\n",
    "#     index = faiss.IndexIVFPQ(index, emb_dim, clusters, m, 8)  # IVF with PQ (100 clusters, 8-bit codes)\n",
    "\n",
    "#     return index\n",
    "\n",
    "def encode_in_batches(texts, embedding_model, batch_size=512):\n",
    "    # Get embedding dimension\n",
    "    sample_emb = embedding_model.encode([texts[0]], device=device)\n",
    "    emb_dim = sample_emb.shape[1] if len(sample_emb.shape) > 1 else sample_emb.shape[0]\n",
    "\n",
    "    num_vectors = len(texts)\n",
    "    \n",
    "    # Dynamically adjust number of clusters\n",
    "    clusters = min(256, max(10, num_vectors // 40))  \n",
    "\n",
    "    if num_vectors < 9984:\n",
    "        print(f\"Dataset has only {num_vectors} vectors. Using IndexFlatL2 instead.\")\n",
    "        index = faiss.IndexFlatL2(emb_dim)  # Flat index (no training needed)\n",
    "    else:\n",
    "        print(f\"Using {clusters} clusters for IVF-PQ.\")\n",
    "\n",
    "        quantizer = faiss.IndexFlatL2(emb_dim)\n",
    "        index = faiss.IndexIVFPQ(quantizer, emb_dim, clusters, 64, 8)\n",
    "\n",
    "        # Collect training data\n",
    "        training_data = []\n",
    "        for i in tqdm(range(0, num_vectors, batch_size), desc=\"Encoding batches (Training)\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            emb = embedding_model.encode(batch_texts, device=device).astype(np.float32)\n",
    "            training_data.append(emb)\n",
    "            if len(training_data) * batch_size >= clusters * 40:\n",
    "                break\n",
    "\n",
    "        training_data = np.vstack(training_data)\n",
    "        print(f\"Training IVF-PQ with {training_data.shape[0]} vectors...\")\n",
    "        index.train(training_data)\n",
    "\n",
    "    # Add vectors\n",
    "    for i in tqdm(range(0, num_vectors, batch_size), desc=\"Encoding batches (Indexing)\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        emb = embedding_model.encode(batch_texts, device=device).astype(np.float32)\n",
    "        index.add(emb)\n",
    "\n",
    "    print(\"Index built successfully!\")\n",
    "    return index\n",
    "\n",
    "def gen_emb_store(df, m_key, cache_dir = './cache'):\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "\n",
    "    # cache_file = os.path.join(cache_dir, f'{m_key}-embeddings.pickle')\n",
    "    cache_file = os.path.join(cache_dir, f\"{m_key}-index.idx\")\n",
    "\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f'Loading embedding from cache: {cache_file}')\n",
    "        # with open(cache_file, 'rb') as f:\n",
    "        #     vector_store = pickle.load(f)\n",
    "        # embed_model = select_model(m_key)\n",
    "\n",
    "        #load the faiss index\n",
    "        vector_store = faiss.read_index(f\"{cache_dir}/{m_key}-index.idx\")\n",
    "        embed_model = select_model(m_key)\n",
    "\n",
    "    else:\n",
    "        print(f'Generating embeddings for model: {m_key}')\n",
    "        embed_model = select_model(m_key)\n",
    "        texts = df['review_text'].tolist()\n",
    "        vector_store = encode_in_batches(texts, embed_model)\n",
    "\n",
    "        faiss.write_index(vector_store, f\"{cache_dir}/{m_key}-index.idx\")\n",
    "\n",
    "    return vector_store, embed_model\n",
    "\n",
    "def compare_models(df, model_keys, cache_dir='./cache'):\n",
    "    results = {}\n",
    "    for m_key in model_keys:\n",
    "        print(f'Processing model: {m_key}')\n",
    "        vector_store, embedder = gen_emb_store(df, m_key, cache_dir)\n",
    "        if vector_store is None:\n",
    "            raise ValueError(f\"Vector store creation failure for {m_key}\")\n",
    "\n",
    "        results[m_key] = (vector_store, embedder)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from tqdm import tqdm\n",
    "\n",
    "def documents_from_df(df, text_field=\"review_text\", batch_size=256):\n",
    "    \"\"\"\n",
    "    Convert a DataFrame into a list of Document objects.\n",
    "    The text_field parameter indicates which column holds the text.\n",
    "    \"\"\"\n",
    "    # Convert each row to a dictionary and use the selected text_field for page_content.\n",
    "    records = df.to_dict(orient=\"records\")\n",
    "    #Documentise the df in BATCHES\n",
    "    docs = []\n",
    "    for idx in tqdm(range(0, len(records), batch_size), desc=\"creating documents\"):\n",
    "        batch_records = records[idx: idx + batch_size]\n",
    "        docs.extend([Document(page_content=rec[text_field], metadata=rec) for rec in batch_records])\n",
    "\n",
    "    return docs\n",
    "\n",
    "    # return [Document(page_content=rec[text_field], metadata=rec) for rec in records]\n",
    "\n",
    "        \n",
    "    # store = LocalFileStore(cache_dir)\n",
    "    # embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    #     embed_model, store, namespace = m_key\n",
    "    # )\n",
    "    # vector_store = FAISS.from_documents(docs, embedder)\n",
    "\n",
    "    # with open(cache_file, 'wb') as f:\n",
    "    #     pickle.dump(vector_store, f)\n",
    "    # print(f'Embeddings saved to cache: {cache_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "if False:\n",
    "    sample_docs = df[:5000]\n",
    "\n",
    "    def time_for_m(m_key, docs):\n",
    "        start_time = time.time()\n",
    "        gen_emb_store(docs, m_key)\n",
    "        end_time = time.time()\n",
    "        return end_time - start_time\n",
    "\n",
    "    num_docs = len(df)\n",
    "    sample_size = len(sample_docs)\n",
    "\n",
    "    for m in models:\n",
    "        time_taken = time_for_m(m, sample_docs)\n",
    "        print(f\"Time taken for {m}: {time_taken} seconds\")\n",
    "        est_time_per_m = (time_taken / sample_size) * num_docs\n",
    "        print(f\"Estimated time for {m} on full dataset: {est_time_per_m} seconds\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "results = compare_models(df, models.keys(), cache_dir='./cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def similarity_search(query, embedding_model, index, k=20):\n",
    "    \"\"\"Perform similarity search using FAISS.\"\"\"\n",
    "    \n",
    "    # Encode query\n",
    "    query_embedding = embedding_model.encode([query], device=device)\n",
    "    \n",
    "    # Ensure correct dtype and shape\n",
    "    query_embedding = np.array(query_embedding, dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "    # Safety checks\n",
    "    if index.ntotal == 0:\n",
    "        raise ValueError(\"FAISS index is empty. Add vectors before querying.\")\n",
    "\n",
    "    if query_embedding.shape[1] != index.d:\n",
    "        raise ValueError(f\"Embedding dimension mismatch: Query({query_embedding.shape[1]}) vs Index({index.d})\")\n",
    "\n",
    "    try:\n",
    "        distances, indices = index.search(query_embedding, k)\n",
    "        return distances, indices\n",
    "    except Exception as e:\n",
    "        print(\"Error in FAISS search:\", e)\n",
    "        return None, None\n",
    "\n",
    "def retrieve_answers(query, index, embedding_model, k=20):\n",
    "    \"\"\"Retrieve top-k similar documents for a query.\"\"\"\n",
    "    distances, indices = similarity_search(query, embedding_model, index, k)\n",
    "\n",
    "    if indices is None:\n",
    "        return []\n",
    "\n",
    "    doc_texts = [df.iloc[i]['review_text'] for i in indices[0] if i >= 0]\n",
    "    return doc_texts\n",
    "\n",
    "# Test similarity search\n",
    "# sample_query = df.iloc[0]['review_text']\n",
    "# sample_index, sample_embedder = results['all-MiniLM-L6-v2']\n",
    "\n",
    "# distances, indices = similarity_search(sample_query, sample_embedder, sample_index)\n",
    "# if distances is not None:\n",
    "#     print(distances, indices)\n",
    "\n",
    "# Test answer query\n",
    "# sample_query = 'What do people like about Spotify?'\n",
    "\n",
    "# for m_key in models.keys():\n",
    "#     answer = retrieve_answers(sample_query, m_key)\n",
    "#     print(f\"Model: {m_key}\")\n",
    "#     print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the quality of documents retrieved and the time it has taken to process, we proceed with the first model: **paraphrase-MiniLM-L6-v2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_model = \"paraphrase-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index,model = gen_emb_store(df, chosen_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: love app hate paying haha 10 bucks great deal listen music everywhere use data fantastic\n",
      "2: cool song nice harmony great lyrics tells like\n",
      "3: using spotify tablets pc smartphone least year best problem got chromecast want stream\n",
      "4: like app shuffle songs artist free music service\n",
      "5: update broken app drive ford option turn stupid ford sync switching google music fixed subscription runs\n",
      "6: sticks artist suggestions allways spot\n",
      "7: thing cant downloaded music storage even though premium\n",
      "8: love app best 9 99 spent ever havent gone back itunes pandora radio since last year\n",
      "9: great app need download songs\n",
      "10: love app much better pandora free customizable playlist sound good anyone\n",
      "11: well like pay listen music offline sucks like favorite songs paying taking far give 3 stars\n",
      "12: love love love making play list awesome\n",
      "13: much variety great\n",
      "14: stupid maker download music tomorrow need download\n",
      "15: got need say friend 2x biz marky\n",
      "16: skookiest choochplayer dere yo\n",
      "17: would like skips pretty amazeballs\n",
      "18: got student discount best 5 bucks month ive ever spent\n",
      "19: thebest resource ever used music nominal monthly fee well worth tailors tastes rather pushing pop 40 downmy throat even obscure tunes love\n",
      "20: ʖ ᕕ ᗜ ᕗ\n",
      "21: different others perfect\n",
      "22: listen every day really cool app\n",
      "23: really good music anywhere go\n",
      "24: love app mind advertising issue recommended songs constantly reccomends songs previously dismissed\n",
      "25: really love far\n",
      "26: love always find music want hear issue lock screen functionality could better great otherwise\n",
      "27: downside toggle shuffle play mode mobile version\n",
      "28: like better iheartradio pandora even commercials\n",
      "29: used skips hear garbage pop music like sometimes trying listen alt folk album hear two back back nicki minaj songs one ad hear one song selected album going make listen songs choose hear least make genre\n",
      "30: time need prince back spotifty please let need prince\n",
      "31: think good hate shuffle feature\n",
      "32: besttttt thing ever created better pandora\n",
      "33: like blue ray connection ps4 phone great list music\n",
      "34: spotipy make day completed\n",
      "35: best music app ever used far\n",
      "36: saves thousands dollars month premium user amazing\n",
      "37: flawless application single issue\n",
      "38: always great sevice say bit pricy 10 per month considerin extras well worth money\n",
      "39: excellent app easy use great variety songs fast find songs like\n",
      "40: great app equalizer could better great unlimited music ads\n",
      "41: started using spotify truly great great music selections different genres\n",
      "42: crashes often new features would like cut version app search play music want recommendations etc\n",
      "43: great music love\n",
      "44: issue lock screen widget id say get rid x easy accidentally hit instead changing songs\n",
      "45: good round little steep price maybe could couple subscription mrs\n",
      "46: love really easy use\n",
      "47: like free music\n",
      "48: past month spotify recognise connect wi fi please fix\n",
      "49: loove app super large database music\n",
      "50: listening spotify since 2009 every year get better better always surprise extremely outstanding quality music offer always look forward discover weekly playlist listen week long new music friday tgif playlists always great way end week much variety playlists provide mood simply best music app universe\n",
      "51: really like app gives exactly need\n",
      "52: hate wont let u listen music offline keeps skippong like cd wont let u pick song u wanna listen u juss shuffle play picks songs u\n",
      "53: find anything want new band rend excellent piped pool greatest\n",
      "54: purchased student premium 5 33 taken account spotify get premium logged back still free 3 hours since purchased premium still received\n",
      "55: amazing app open reason closes one time done\n",
      "56: love songs love awesome\n",
      "57: amazing app\n",
      "58: app great generally works perfectly thing would like little choice radio stations\n",
      "59: amazing awesome\n",
      "60: great link spotify sonos system\n",
      "61: use paid service access many artist listen amazing\n",
      "62: cool make kind play list enjoy offline\n",
      "63: really good exept commercials\n",
      "64: people might say missing musicians still believe go deep need\n",
      "65: absolutely best free music app favorite although mobile ads limited skips unless premium tablets hardly ads zero limitations number skips amazing get always uninstall lol love spotify\n",
      "66: musician music enthusiast best investment made\n",
      "67: dosent really songs like also dont like way always shuffle dont want pay 9 per month either\n",
      "68: music never messed ads long short\n",
      "69: nice app like free app anything want beneficial requires upgrade annoying hey\n",
      "70: issues great music great variety love app always go music\n",
      "71: lets pick music wanna hear pay extra play song want might well stick pandora skips spotify like 1 half skips unless pay aint nobody got time fo dat\n",
      "72: love spotify everytime try upgrade ad free music redirects ad tells much life sucks without upgrade without giving way upgrade annoying\n",
      "73: really like spotify diverse selection songs however easily accessible without premium get spotify would suggest also getting premium best\n",
      "74: downloaded music almost every title search clean interface\n",
      "75: tried say spotify far best keep good work never stop innovating\n",
      "76: usse friendly versatile music selections\n",
      "77: coba ada liriknya makin mantap lah\n",
      "78: never ceases amaze love geared work outs well leisure love app\n",
      "79: overall good like much\n",
      "80: every album need right short thats great app\n",
      "81: love spotify always newest remixes artists overall easy use convenient\n",
      "82: haldane lovvd app viva\n",
      "83: love great music app\n",
      "84: good shame gave pay\n",
      "85: great way listen music whole world music one tap away\n",
      "86: great way listen variety random songs love\n",
      "87: great app change log would nice rolling updates previous change logs humorous least\n",
      "88: awesome collections music esp praise worship songs\n",
      "89: u premium play song u want screw\n",
      "90: love spotify know te artist sing u want create playlist also many adds swiching new spotify still good app\n",
      "91: application good\n",
      "92: never use anything else\n",
      "93: would make much better\n",
      "94: would give 5 would let download music even wifi reason letting im getting mad cause want listen music uhhhhhhhhghghghg please respond\n",
      "95: go app favorite music\n",
      "96: big variety music better sound cloud\n",
      "97: great app suggestion would option available turn without logging would also nice able play one song able start song\n",
      "98: mostly use app control music desktop application using full screen software\n",
      "99: issues good sound wide variety music\n",
      "100: let login fb account ether says problem throws trying login different country yes move need inform every spotify employee moved move please awful going jango 2nd time issue spotify 1st time still waiting answer\n"
     ]
    }
   ],
   "source": [
    "# Test the retrevial of chosen_model\n",
    "sample_query = 'What do people like about Spotify?'\n",
    "\n",
    "\n",
    "answer = retrieve_answers(sample_query, index, model,k=100)\n",
    "for i,ans in enumerate(answer):\n",
    "    print(f\"{i+1}: {ans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding LLM and a building retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache file: .\\cache\\paraphrase-MiniLM-L6-v2-embeddings.pickle\n",
      "Loading embedding from cache: .\\cache\\paraphrase-MiniLM-L6-v2-embeddings.pickle\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "def batch_add(docs, embedder, batch_size=512):\n",
    "    batch = docs[0:batch_size]\n",
    "    vector_store = FAISS.from_documents(batch, embedder)\n",
    "    print('one batch added')\n",
    "    for i in tqdm(range(batch_size, len(docs), batch_size), desc=\"Adding batches\"):\n",
    "        batch = docs[i:i + batch_size]\n",
    "        temp = FAISS.from_documents(batch, embedder)\n",
    "        vector_store.merge_from(temp)\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "cache_dir = \"cache\"\n",
    "cache_file = os.path.join(\".\", cache_dir, f\"{chosen_model}-embeddings.pickle\")\n",
    "print(f\"Cache file: {cache_file}\")\n",
    "embed_model = HuggingFaceEmbeddings(model_name=f\"{chosen_model}\", model_kwargs={\"device\": \"cuda\"})\n",
    "\n",
    "if os.path.exists(cache_file):\n",
    "    print(f'Loading embedding from cache: {cache_file}')\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        vector_store = pickle.load(f)\n",
    "        \n",
    "else:\n",
    "    print(f'Generating embeddings for model: {chosen_model}')\n",
    "    store = LocalFileStore(cache_dir)\n",
    "    embedder = CacheBackedEmbeddings.from_bytes_store(embed_model, store, namespace=f\"{chosen_model}\")\n",
    "    print(\"embedder loaded\")\n",
    "    docs = documents_from_df(df)\n",
    "    print(\"generated docs\")\n",
    "    vector_store = batch_add(docs, embedder)\n",
    "\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(vector_store, f)\n",
    "    print(f'Embeddings saved to cache: {cache_file}')\n",
    "\n",
    "# vector_store = results['all-MiniLM-L6-v2'][0]\n",
    "# documents = documents_from_df(df)\n",
    "# doc_dict = {i: doc for i, doc in enumerate(documents)}\n",
    "# doc_store = InMemoryDocstore({i: doc for i, doc in enumerate(documents)})\n",
    "# index_to_docstore_id = {i: i for i in range(len(documents))}\n",
    "pathlib.PosixPath = temp\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# torch.backends.cuda.enable_flash_sdp(True)\n",
    "# torch.backends.cuda.enable_math_sdp(False)\n",
    "# torch.backends.cuda.enable_mem_efficient_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# llm_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# llm_model = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "llm_model = \"../model\"\n",
    "\n",
    "# hf_token = os.getenv('HUGGINGFACE_HUB_TOKEN') \n",
    "# login(token=hf_token)\n",
    "\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(llm_model)\n",
    "\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        llm_model,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float16       # use float16 to reduce memory footprint\n",
    "    )\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    return_full_text=True,\n",
    "    temperature=0.5,\n",
    "    max_new_tokens=250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "User question: {question}\n",
    "\n",
    "Answer the user's question using the following context.\n",
    "\n",
    "Follow these rules in order to answer the user's question:\n",
    "1) Your answer should be short (maximum 3 short coherent sentences).\n",
    "2) Use the dataset information on Google Store reviews for Spotify to extract actionable insights.\n",
    "3) Your answer should be a coherent question answering the question with the given context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Your answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template = prompt_template, input_variables=[\"question\", \"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    #verbose=True,\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanw\\AppData\\Local\\Temp\\ipykernel_20884\\202821713.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa({\"question\": query, \"chat_history\": []})\n",
      "c:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User question: What do people like about Spotify?\n",
      "\n",
      "Answer the user's question using the following context.\n",
      "\n",
      "Follow these rules in order to answer the user's question:\n",
      "1) Your answer should be short (maximum 3 short coherent sentences).\n",
      "2) Use the dataset information on Google Store reviews for Spotify to extract actionable insights.\n",
      "3) Your answer should be a coherent question answering the question with the given context.\n",
      "\n",
      "Context:\n",
      "really like enjoy spotify\n",
      "\n",
      "really like spotify cool\n",
      "\n",
      "really like spotify complaints\n",
      "\n",
      "really enjoy spotify best\n",
      "\n",
      "Your answer:\n",
      "I like the music quality, it's better than what I expected, and the layout is also nice.\n",
      "\n",
      "I enjoy the music and the layout is good.\n",
      "\n",
      "I really like the music quality and the layout is nice.\n",
      "\n",
      "I really like the music and the layout is good.\n",
      "\n",
      "I really like the music and the layout is nice.\n",
      "\n",
      "I really like the music and the layout is good.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is\n"
     ]
    }
   ],
   "source": [
    "query = \"What do people like about Spotify?\"\n",
    "result = qa({\"question\": query, \"chat_history\": []})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def truncate_context(context):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "    max_tokens = 510  # leave space for special tokens and question tokens\n",
    "\n",
    "    tokens = tokenizer.encode(context, add_special_tokens=False)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        context = tokenizer.decode(tokens)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_text_from_query(query, context):\n",
    "\n",
    "    model = transformers.pipeline(\n",
    "        task=\"question-answering\",\n",
    "        model=\"distilbert-base-cased-distilled-squad\",\n",
    "        tokenizer=tokenizer,\n",
    "        # task=\"text-generation\",\n",
    "        return_full_text=True,\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=510\n",
    "    )\n",
    "\n",
    "    context = truncate_context(context)\n",
    "    # Pass both values to the QA pipeline\n",
    "    return model(question=query, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding from cache: ./cache\\paraphrase-MiniLM-L6-v2-index.idx\n"
     ]
    }
   ],
   "source": [
    "index, model = gen_emb_store(df, chosen_model, cache_dir='./cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_text_from_query(query, context)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Then run your query\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43manswer_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat do people like about Spotify?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m, in \u001b[0;36manswer_query\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      2\u001b[0m retrevial_result \u001b[38;5;241m=\u001b[39m retrieve_answers(query, index, model)\n\u001b[0;32m      3\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(retrevial_result)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate_text_from_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 15\u001b[0m, in \u001b[0;36mgenerate_text_from_query\u001b[1;34m(query, context)\u001b[0m\n\u001b[0;32m     13\u001b[0m context \u001b[38;5;241m=\u001b[39m truncate_context(context)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Pass both values to the QA pipeline\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:398\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args_parser(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:1293\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[1;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:1208\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1207\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1208\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1209\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:525\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline._forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    524\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 525\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m\"\u001b[39m: example, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs}\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:1100\u001b[0m, in \u001b[0;36mDistilBertForQuestionAnswering.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1100\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1109\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, max_query_len, dim)\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)  \u001b[38;5;66;03m# (bs, max_query_len, dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:794\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    791\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(input_shape, device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# (bs, seq_length)\u001b[39;00m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[1;32m--> 794\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_4d_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m    799\u001b[0m     x\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m    800\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    804\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    805\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:444\u001b[0m, in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[1;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[0;32m    441\u001b[0m is_tracing \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mProxy) \u001b[38;5;129;01mor\u001b[39;00m is_torchdynamo_compiling()\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# torch.jit.trace, symbolic_trace and torchdynamo with fullgraph=True are unable to capture data-dependent controlflows.\u001b[39;00m\n\u001b[1;32m--> 444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def answer_query(query):\n",
    "    retrevial_result = retrieve_answers(query, index, model)\n",
    "    context = \" \".join(retrevial_result)\n",
    "    return generate_text_from_query(query, context)\n",
    "\n",
    "# Then run your query\n",
    "print(answer_query(\"What do people like about Spotify?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_query(\"What do users indiate they like about Spotify?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
