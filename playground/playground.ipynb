{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from llama_index import (Document)\n",
    "\n",
    "# Load documents from the review dataset directory (adjust path as needed)\n",
    "csv_path = \"../Data/SPOTIFY_REVIEWS.csv\"\n",
    "df = pd.read_csv(csv_path, usecols=[\"review_text\",\"review_rating\",\"review_likes\"], nrows=500000)\n",
    "# df = df[:1000000]\n",
    "\n",
    "#lambda col: col not in [\"author_name\", \"author_app_version\",\"review_timestamp\"]\n",
    "# docs = [Document(text=row[\"review_text\"])\n",
    "#         for index, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i enjoy the awesome ui of this app and it has ...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love it especially the new design</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>awesome ui best music app out there</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as a professional android developer i m glad t...</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>if i had to put a amount on how much i would h...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  review_rating  \\\n",
       "1  i enjoy the awesome ui of this app and it has ...              5   \n",
       "2                 love it especially the new design               5   \n",
       "3               awesome ui best music app out there               5   \n",
       "4  as a professional android developer i m glad t...              5   \n",
       "5  if i had to put a amount on how much i would h...              5   \n",
       "\n",
       "   review_likes  \n",
       "1             4  \n",
       "2             2  \n",
       "3             1  \n",
       "4            10  \n",
       "5             4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rows with review text less than 5 tokens\n",
    "def remove_short_lines(df):\n",
    "    return df.loc[df[\"review_text\"].str.count(\" \") >= 4]\n",
    "\n",
    "def preprocess_text(df):\n",
    "    df.drop_duplicates(subset=[\"review_text\"], inplace=True)\n",
    "    df.dropna(subset=[\"review_text\"], inplace=True)\n",
    "    df[\"review_text\"] = (\n",
    "        df[\"review_text\"]\n",
    "        .str.replace(r'\\W+', ' ', regex=True)  # replace non-alphanumeric with space\n",
    "        .str.replace(r'\\s+', ' ', regex=True)  # collapse whitespace\n",
    "        .str.lower()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df = remove_short_lines(df)\n",
    "df = preprocess_text(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "# Load SymSpell dictionary (ensure you have \"frequency_dictionary_en.txt\")\n",
    "dictionary_path = importlib.resources.files(\"symspellpy\") / \"frequency_dictionary_en_82_765.txt\"\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2)\n",
    "sym_spell.load_dictionary(str(dictionary_path), term_index=0, count_index=1)\n",
    "\n",
    "def correct_spelling(text):\n",
    "    suggestions = sym_spell.lookup(text, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "    return suggestions[0].term if suggestions else text\n",
    "\n",
    "df[\"review_text\"] = df[\"review_text\"].apply(lambda x: correct_spelling(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(df):\n",
    "    df['review_text'] = df['review_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "remove_stopwords(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.12/site-packages/cudf/utils/_ptxcompiler.py:64: UserWarning: Error getting driver and runtime versions:\n",
      "\n",
      "stdout:\n",
      "\n",
      "\n",
      "\n",
      "stderr:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 7, in <module>\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/numba_cuda/numba/cuda/cudadrv/runtime.py\", line 111, in get_version\n",
      "    self.cudaRuntimeGetVersion(ctypes.byref(rtver))\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/numba_cuda/numba/cuda/cudadrv/runtime.py\", line 65, in __getattr__\n",
      "    self._initialize()\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/numba_cuda/numba/cuda/cudadrv/runtime.py\", line 51, in _initialize\n",
      "    self.lib = open_cudalib('cudart')\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vscode/.local/lib/python3.12/site-packages/numba_cuda/numba/cuda/cudadrv/libs.py\", line 84, in open_cudalib\n",
      "    return ctypes.CDLL(path)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/ctypes/__init__.py\", line 379, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: libcudart.so: cannot open shared object file: No such file or directory\n",
      "\n",
      "\n",
      "Not patching Numba\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to dlopen libcudart.so.12",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer \u001b[38;5;28;01mas\u001b[39;00m cuTfidfVectorizer\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pairwise_distances\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/cuml/__init__.py:27\u001b[39m\n\u001b[32m     24\u001b[39m     libcuml.load_library()\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m libcuml\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Base, UniversalBase\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mavailable_devices\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_cuda_available\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# GPU only packages\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/cuml/internals/__init__.py:18\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Copyright (c) 2019-2023, NVIDIA CORPORATION.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mavailable_devices\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_cuda_available\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase_helpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseMetaClass, _tags_class_and_instance\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_decorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     _deprecate_pos_args,\n\u001b[32m     21\u001b[39m     api_base_fit_transform,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     exit_internal_api,\n\u001b[32m     34\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_context_managers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     36\u001b[39m     in_internal_api,\n\u001b[32m     37\u001b[39m     set_api_output_dtype,\n\u001b[32m     38\u001b[39m     set_api_output_type,\n\u001b[32m     39\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/cuml/internals/base_helpers.py:20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Parameter, signature\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_decorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     21\u001b[39m     api_base_return_generic,\n\u001b[32m     22\u001b[39m     api_base_return_array,\n\u001b[32m     23\u001b[39m     api_base_return_sparse_array,\n\u001b[32m     24\u001b[39m     api_base_return_any,\n\u001b[32m     25\u001b[39m     api_return_any,\n\u001b[32m     26\u001b[39m     _deprecate_pos_args,\n\u001b[32m     27\u001b[39m )\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CumlArray\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_sparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseCumlArray\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/cuml/internals/api_decorators.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# TODO: Try to resolve circular import that makes this necessary:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m input_utils \u001b[38;5;28;01mas\u001b[39;00m iu\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_context_managers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseReturnAnyCM\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_context_managers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseReturnArrayCM\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/cuml/internals/input_utils.py:20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m namedtuple\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CumlArray\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_sparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseCumlArray\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mglobal_settings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalSettings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/cuml/internals/array.py:21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moperator\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mglobal_settings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalSettings\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m debug\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmem_type\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MemoryType, MemoryTypeError\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/cuml/internals/global_settings.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mthreading\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mavailable_devices\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_cuda_available\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdevice_type\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeviceType\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmem_type\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MemoryType\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msafe_imports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cpu_only_import, gpu_only_import\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/cuml/internals/device_type.py:19\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Copyright (c) 2022-2023, NVIDIA CORPORATION.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Enum, auto\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmem_type\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MemoryType\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mDeviceTypeError\u001b[39;00m(\u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[32m     23\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"An exception thrown to indicate bad device type selection\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/cuml/internals/mem_type.py:22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdevice_support\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPU_ENABLED\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msafe_imports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cpu_only_import, gpu_only_import\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m cudf = \u001b[43mgpu_only_import\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcudf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m cp = gpu_only_import(\u001b[33m\"\u001b[39m\u001b[33mcupy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m cpx_sparse = gpu_only_import(\u001b[33m\"\u001b[39m\u001b[33mcupyx.scipy.sparse\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/cuml/internals/safe_imports.py:362\u001b[39m, in \u001b[36mgpu_only_import\u001b[39m\u001b[34m(module, alt)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"A function used to import modules required only in GPU installs\u001b[39;00m\n\u001b[32m    337\u001b[39m \n\u001b[32m    338\u001b[39m \u001b[33;03mThis function will attempt to import a module with the given name, but it\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    359\u001b[39m \u001b[33;03m    UnavailableMeta.\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m GPU_ENABLED:\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_import(\n\u001b[32m    365\u001b[39m         module,\n\u001b[32m    366\u001b[39m         msg=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not installed in non GPU-enabled installations\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    367\u001b[39m         alt=alt,\n\u001b[32m    368\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/cudf/__init__.py:20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcudf\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgpu_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_setup\n\u001b[32m     19\u001b[39m _setup_numba()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mvalidate_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcupy\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config \u001b[38;5;28;01mas\u001b[39;00m numba_config, cuda\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/cudf/utils/gpu_utils.py:96\u001b[39m, in \u001b[36mvalidate_setup\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     86\u001b[39m     minor_version = getDeviceAttribute(\n\u001b[32m     87\u001b[39m         cudaDeviceAttr.cudaDevAttrComputeCapabilityMinor, \u001b[32m0\u001b[39m\n\u001b[32m     88\u001b[39m     )\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnsupportedCUDAError(\n\u001b[32m     90\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA GPU with NVIDIA Voltaâ„¢ (Compute Capability 7.0) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     91\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor newer architecture is required.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     92\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDetected GPU 0: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     93\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDetected Compute Capability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmajor_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminor_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     94\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m cuda_runtime_version = \u001b[43mruntimeGetVersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cuda_runtime_version < \u001b[32m11000\u001b[39m:\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# Require CUDA Runtime version 11.0 or greater.\u001b[39;00m\n\u001b[32m    100\u001b[39m     major_version = cuda_runtime_version // \u001b[32m1000\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/rmm/_cuda/gpu.py:86\u001b[39m, in \u001b[36mruntimeGetVersion\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mruntimeGetVersion\u001b[39m():\n\u001b[32m     77\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m    Returns the version number of the local CUDA runtime.\u001b[39;00m\n\u001b[32m     79\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m \u001b[33;03m    and status code.\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     status, version = \u001b[43mruntime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetLocalRuntimeVersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != runtime.cudaError_t.cudaSuccess:\n\u001b[32m     88\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CUDARuntimeError(status)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/cuda/bindings/runtime.pyx:31996\u001b[39m, in \u001b[36mcuda.bindings.runtime.getLocalRuntimeVersion\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/cuda/bindings/cyruntime.pyx:1225\u001b[39m, in \u001b[36mcuda.bindings.cyruntime.getLocalRuntimeVersion\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/cuda/bindings/_lib/cyruntime/cyruntime.pyx:4005\u001b[39m, in \u001b[36mcuda.bindings._lib.cyruntime.cyruntime._getLocalRuntimeVersion\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to dlopen libcudart.so.12"
     ]
    }
   ],
   "source": [
    "import cudf\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as cuTfidfVectorizer\n",
    "from cuml.metrics import pairwise_distances\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def vectorized_deduplicate_dataframe(df, text_column=\"review_text\", threshold=0.8):\n",
    "    \"\"\"\n",
    "    Efficiently deduplicates a dataframe based on text similarity using TF-IDF and cosine similarity on GPU using RAPIDS cuML.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): Input DataFrame containing text data.\n",
    "    - text_column (str): Column name of text data.\n",
    "    - threshold (float): Similarity threshold (0 to 1), where higher means stricter deduplication.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Deduplicated DataFrame with all original columns preserved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute TF-IDF embeddings on GPU\n",
    "    vectorizer = cuTfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix_gpu = vectorizer.fit_transform(df[text_column])\n",
    "\n",
    "    print(\"Fitted matrix\")\n",
    "\n",
    "    # Normalize the matrix (already normalized in cuML TF-IDF)\n",
    "    print(\"Normalized matrix\")\n",
    "\n",
    "    # Compute cosine similarity matrix on GPU\n",
    "    similarity_matrix_gpu = pairwise_distances(tfidf_matrix_gpu, tfidf_matrix_gpu, metric=\"cosine\")\n",
    "    print(\"Computed similarity matrix on GPU\")\n",
    "\n",
    "    # Identify duplicates\n",
    "    unique_indexes = []\n",
    "    seen = set()\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        if i in seen:\n",
    "            continue\n",
    "        # Find similar reviews\n",
    "        similar_indexes = pairwise_distances(similarity_matrix_gpu[i], similarity_matrix_gpu[i], metric=\"cosine\").toarray()\n",
    "        similar_indexes = [idx for idx, val in enumerate(similar_indexes) if val > threshold]\n",
    "        seen.update(similar_indexes)  # Mark them as seen\n",
    "        unique_indexes.append(i)  # Keep only the first occurrence\n",
    "\n",
    "    # Return deduplicated DataFrame\n",
    "    return df.iloc[unique_indexes].reset_index(drop=True)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df_dedup1 = vectorized_deduplicate_dataframe(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import cupy as cp\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "def vectorized_deduplicate_dataframe(df, text_column=\"review_text\", threshold=0.8):\n",
    "    \"\"\"\n",
    "    Efficiently deduplicates a dataframe based on text similarity using TF-IDF and cosine similarity on GPU.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): Input DataFrame containing text data.\n",
    "    - text_column (str): Column name of text data.\n",
    "    - threshold (float): Similarity threshold (0 to 1), where higher means stricter deduplication.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Deduplicated DataFrame with all original columns preserved.\n",
    "    \"\"\"\n",
    "    # Compute TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[text_column])\n",
    "\n",
    "    print(\"Fitted matrix\")\n",
    "\n",
    "    # Convert the sparse matrix to a dense matrix and transfer it to GPU\n",
    "    tfidf_matrix_gpu = cp.sparse.csr_matrix(tfidf_matrix).todense()\n",
    "    tfidf_matrix_gpu = cp.asarray(tfidf_matrix_gpu)\n",
    "\n",
    "    # Normalize the matrix to unit vectors (L2 normalization)\n",
    "    tfidf_matrix_gpu /= cp.linalg.norm(tfidf_matrix_gpu, axis=1)[:, cp.newaxis]\n",
    "    print(\"Normalized matrix\")\n",
    "\n",
    "    # Compute the cosine similarity matrix on GPU\n",
    "    similarity_matrix_gpu = cp.matmul(tfidf_matrix_gpu, tfidf_matrix_gpu.T)\n",
    "    print(\"Computed similarity matrix on GPU\")\n",
    "\n",
    "    # Identify duplicates\n",
    "    unique_indexes = []\n",
    "    seen = set()\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        if i in seen:\n",
    "            continue\n",
    "        # Find similar reviews\n",
    "        similar_indexes = cp.where(similarity_matrix_gpu[i] > threshold)[0].get()  # Transfer results to CPU\n",
    "        seen.update(similar_indexes)  # Mark them as seen\n",
    "        unique_indexes.append(i)  # Keep only the first occurrence\n",
    "\n",
    "    # Return the deduplicated DataFrame\n",
    "    return df.iloc[unique_indexes].reset_index(drop=True)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df_dedup1 = vectorized_deduplicate_dataframe(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "def vectorized_deduplicate_dataframe(df, text_column=\"review_text\", threshold=0.8):\n",
    "    \"\"\"\n",
    "    Efficiently deduplicates a dataframe based on text similarity using TF-IDF and cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): Input DataFrame containing text data.\n",
    "    - text_column (str): Column name of text data.\n",
    "    - threshold (float): Similarity threshold (0 to 1), where higher means stricter deduplication.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Deduplicated DataFrame with all original columns preserved.\n",
    "    \"\"\"\n",
    "    # Compute TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[text_column])\n",
    "    print(\"Fitted matrix\")\n",
    "\n",
    "    tfidf_matrix = normalize(tfidf_matrix, norm=\"l2\")\n",
    "    print(\"normalised\")\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    print(\"Computed similarity matrix\")\n",
    "\n",
    "    # Identify duplicates\n",
    "    unique_indexes = []\n",
    "    seen = set()\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        if i in seen:\n",
    "            continue\n",
    "        # Find similar reviews\n",
    "        similar_indexes = np.where(similarity_matrix[i] > threshold)[0]\n",
    "        seen.update(similar_indexes)  # Mark them as seen\n",
    "        unique_indexes.append(i)  # Keep only the first occurrence\n",
    "\n",
    "    # Return deduplicated DataFrame\n",
    "    return df.iloc[unique_indexes].reset_index(drop=True)\n",
    "\n",
    "df_dedup1 = vectorized_deduplicate_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show 30 lines with length 5\n",
    "df[df[\"review_text\"].str.count(\" \") == 4].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from textblob import TextBlob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('punkt')\n",
    "\n",
    "df['num_tokens'] = df['review_text'].apply(lambda text: len(TextBlob(text).words))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['num_tokens'], kde=True, bins=50, color='blue')\n",
    "plt.xlabel('Number of tokens per review')\n",
    "plt.ylabel('Frequency')\n",
    "df.drop(columns=['num_tokens'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the reviews are quite short, we do not need to split them using text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# module_name = \"faiss\"\n",
    "# if module_name in sys.modules:\n",
    "#     del sys.modules[module_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import faiss\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "models = {\n",
    "    'all-MiniLM-L6-v2': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    'multi-qa-MiniLM-L6-dot-v1': 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1',\n",
    "    'paraphrase-MiniLM-L6-v2': 'sentence-transformers/paraphrase-MiniLM-L6-v2',\n",
    "}\n",
    "\n",
    "def select_model(m_key):\n",
    "    if m_key not in models:\n",
    "        raise ValueError(f'Model key is not found')\n",
    "    return SentenceTransformer(m_key, device=device)\n",
    "\n",
    "# import faiss\n",
    "# import torch\n",
    "\n",
    "# # Assume you have an index (e.g. a flat index on CPU)\n",
    "# cpu_index = faiss.IndexFlatL2(128)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"Using FAISS GPU\")\n",
    "#     # Create GPU resources and transfer the index to GPU 0\n",
    "#     res = faiss.StandardGpuResources()\n",
    "#     gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "#     index = gpu_index\n",
    "# else:\n",
    "#     print(\"Using FAISS CPU\")\n",
    "#     index = cpu_index\n",
    "\n",
    "# # Now use `index` for your similarity search\n",
    "\n",
    "# def encode_in_batches(texts, embedding_model, batch_size=512):\n",
    "#     sample_emb = embedding_model.encode([df.iloc[0]['review_text']], device=device)\n",
    "#     emb_dim = len(sample_emb[0])\n",
    "\n",
    "#     index = faiss.IndexFlatL2(emb_dim)\n",
    "\n",
    "#     for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding batches\"):\n",
    "#         batch_texts = texts[i:i + batch_size]\n",
    "#         emb = embedding_model.encode(batch_texts, device=device)\n",
    "#         index.add(emb)\n",
    "\n",
    "#     print(\"Compressing index\")\n",
    "#     m = 64  # Number of sub-vectors\n",
    "#     clusters = 100  # Number of clusters\n",
    "#     index = faiss.IndexIVFPQ(index, emb_dim, clusters, m, 8)  # IVF with PQ (100 clusters, 8-bit codes)\n",
    "\n",
    "#     return index\n",
    "\n",
    "def encode_in_batches(texts, embedding_model, batch_size=512):\n",
    "    # Get embedding dimension\n",
    "    sample_emb = embedding_model.encode([texts[0]], device=device)\n",
    "    emb_dim = sample_emb.shape[1] if len(sample_emb.shape) > 1 else sample_emb.shape[0]\n",
    "\n",
    "    num_vectors = len(texts)\n",
    "    \n",
    "    # Dynamically adjust number of clusters\n",
    "    clusters = min(256, max(10, num_vectors // 40))  \n",
    "\n",
    "    if num_vectors < 9984:\n",
    "        print(f\"Dataset has only {num_vectors} vectors. Using IndexFlatL2 instead.\")\n",
    "        index = faiss.IndexFlatL2(emb_dim)  # Flat index (no training needed)\n",
    "    else:\n",
    "        print(f\"Using {clusters} clusters for IVF-PQ.\")\n",
    "\n",
    "        quantizer = faiss.IndexFlatL2(emb_dim)\n",
    "        index = faiss.IndexIVFPQ(quantizer, emb_dim, clusters, 64, 8)\n",
    "\n",
    "        # Collect training data\n",
    "        training_data = []\n",
    "        for i in tqdm(range(0, num_vectors, batch_size), desc=\"Encoding batches (Training)\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            emb = embedding_model.encode(batch_texts, device=device).astype(np.float32)\n",
    "            training_data.append(emb)\n",
    "            if len(training_data) * batch_size >= clusters * 40:\n",
    "                break\n",
    "\n",
    "        training_data = np.vstack(training_data)\n",
    "        print(f\"Training IVF-PQ with {training_data.shape[0]} vectors...\")\n",
    "        index.train(training_data)\n",
    "\n",
    "    # Add vectors\n",
    "    for i in tqdm(range(0, num_vectors, batch_size), desc=\"Encoding batches (Indexing)\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        emb = embedding_model.encode(batch_texts, device=device).astype(np.float32)\n",
    "        index.add(emb)\n",
    "\n",
    "    print(\"Index built successfully!\")\n",
    "    return index\n",
    "\n",
    "def gen_emb_store(df, m_key, cache_dir = './cache'):\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "\n",
    "    # cache_file = os.path.join(cache_dir, f'{m_key}_embeddings.pickle')\n",
    "    cache_file = os.path.join(cache_dir, f\"{m_key}_index.idx\")\n",
    "\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f'Loading embedding from cache: {cache_file}')\n",
    "        # with open(cache_file, 'rb') as f:\n",
    "        #     vector_store = pickle.load(f)\n",
    "        # embed_model = select_model(m_key)\n",
    "\n",
    "        #load the faiss index\n",
    "        vector_store = faiss.read_index(f\"{cache_dir}/{m_key}_index.idx\")\n",
    "        embed_model = select_model(m_key)\n",
    "\n",
    "    else:\n",
    "        print(f'Generating embeddings for model: {m_key}')\n",
    "        embed_model = select_model(m_key)\n",
    "        texts = df['review_text'].tolist()\n",
    "        vector_store = encode_in_batches(texts, embed_model)\n",
    "\n",
    "        faiss.write_index(vector_store, f\"{cache_dir}/{m_key}_index.idx\")\n",
    "\n",
    "    return vector_store, embed_model\n",
    "\n",
    "def compare_models(df, model_keys, cache_dir='./cache'):\n",
    "    results = {}\n",
    "    for m_key in model_keys:\n",
    "        print(f'Processing model: {m_key}')\n",
    "        vector_store, embedder = gen_emb_store(df, m_key, cache_dir)\n",
    "        if vector_store is None:\n",
    "            raise ValueError(f\"Vector store creation failure for {m_key}\")\n",
    "\n",
    "        results[m_key] = (vector_store, embedder)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documents_from_df(df, text_field=\"review_text\", batch_size=256):\n",
    "    \"\"\"\n",
    "    Convert a DataFrame into a list of Document objects.\n",
    "    The text_field parameter indicates which column holds the text.\n",
    "    \"\"\"\n",
    "    # Convert each row to a dictionary and use the selected text_field for page_content.\n",
    "    records = df.to_dict(orient=\"records\")\n",
    "    #Documentise the df in BATCHES\n",
    "    docs = []\n",
    "    for idx in tqdm(range(0, len(records), batch_size), desc=\"creating documents\"):\n",
    "        batch_records = records[idx: idx + batch_size]\n",
    "        docs.extend([Document(page_content=rec[text_field], metadata=rec) for rec in batch_records])\n",
    "\n",
    "    return docs\n",
    "\n",
    "    # return [Document(page_content=rec[text_field], metadata=rec) for rec in records]\n",
    "\n",
    "        \n",
    "    # store = LocalFileStore(cache_dir)\n",
    "    # embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    #     embed_model, store, namespace = m_key\n",
    "    # )\n",
    "    # vector_store = FAISS.from_documents(docs, embedder)\n",
    "\n",
    "    # with open(cache_file, 'wb') as f:\n",
    "    #     pickle.dump(vector_store, f)\n",
    "    # print(f'Embeddings saved to cache: {cache_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "if False:\n",
    "    sample_docs = df[:5000]\n",
    "\n",
    "    def time_for_m(m_key, docs):\n",
    "        start_time = time.time()\n",
    "        gen_emb_store(docs, m_key)\n",
    "        end_time = time.time()\n",
    "        return end_time - start_time\n",
    "\n",
    "    num_docs = len(df)\n",
    "    sample_size = len(sample_docs)\n",
    "\n",
    "    for m in models:\n",
    "        time_taken = time_for_m(m, sample_docs)\n",
    "        print(f\"Time taken for {m}: {time_taken} seconds\")\n",
    "        est_time_per_m = (time_taken / sample_size) * num_docs\n",
    "        print(f\"Estimated time for {m} on full dataset: {est_time_per_m} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing model: all-MiniLM-L6-v2\n",
      "Loading embedding from cache: ./cache/all-MiniLM-L6-v2_index.idx\n",
      "Processing model: multi-qa-MiniLM-L6-dot-v1\n",
      "Loading embedding from cache: ./cache/multi-qa-MiniLM-L6-dot-v1_index.idx\n",
      "Processing model: paraphrase-MiniLM-L6-v2\n",
      "Loading embedding from cache: ./cache/paraphrase-MiniLM-L6-v2_index.idx\n"
     ]
    }
   ],
   "source": [
    "results = compare_models(df, models.keys(), cache_dir='./cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: all-MiniLM-L6-v2\n",
      "['incredible what is there to not like about spotify ', 'i like every single thing about spotify ', 'i love everything about spotify ', 'i love everything about spotify ', 'i really like the user experience with spotify ', 'i have no complaints i love spotify ', 'i love spotify no two ways about it ', 'i absolutely love spotify ', 'just started using spotify but liking it so far ', 'i enjoy the easy and uncomplicated nature of spotify ', 'i just love spotify it s great ', 'been using spotify over 2 years now and i just love it ', 'been using spotify for a couple years and absolutely love it ', 'i love spotify it is amasing', 'i love spotify i use it everyday ', 'i love spotify its so cool', 'i have no complaints spotify is great ', 'i really like spotify i ve had no trouble with it it s awesome ', 'i am thoroughly enjoying spotify ', 'i love spotify no complaints honestly i love the different things you can do it gives different variety']\n",
      "Model: multi-qa-MiniLM-L6-dot-v1\n",
      "['i like every single thing about spotify ', 'i enjoy spotify very much ', 'i love everything about spotify ', 'i love everything about spotify ', 'i like this spotify ', 'i love spotify and recommend it to all my friends ', 'i love spotify a lot', 'the spotify is so nice so i like it', 'i really like and enjoy spotify ', 'i love spotify its so convinient ', 'i love spotify i recommend it to all of my friends ', 'i love the spotify because ', 'i love spotify it s great ', 'i love spotify it s great', 'oh how i love spotify', 'i love spotify it is great', 'i love spotify it is amasing', 'i am thoroughly enjoying spotify ', 'i really love spotify that s all ', 'i like it so well this spotify ']\n",
      "Model: paraphrase-MiniLM-L6-v2\n",
      "['i really like the user experience with spotify ', 'i really like and enjoy spotify ', 'my daughter and i really like spotify and its flexibility ', 'i love spotify a lot', 'i love spotify so much why are you people complaining', 'i really like spotify because idk i just do', 'what s not to love about spotify thanks guys for an excellent product ', 'i love spotify it s awesome for someone who listens to a lot of music ', 'i love spotify i recommend it to all of my friends ', 'oh how i love spotify', 'i love spotify and recommend it to all my friends ', 'i like this spotify ', 'i like spotify it s great', 'i get to listen to what i like when i d like to live spotify ', 'i like every single thing about spotify ', 'i really like spotify strongly recommend ', 'spotify is awesome i like it ', 'i enjoy spotify very much ', 'i love spotify because i am in charge of what i listen to ', 'i love spotify i like']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def similarity_search(query, embedding_model, index, k=20):\n",
    "    \"\"\"Perform similarity search using FAISS.\"\"\"\n",
    "    \n",
    "    # Encode query\n",
    "    query_embedding = embedding_model.encode([query], device=device)\n",
    "    \n",
    "    # Ensure correct dtype and shape\n",
    "    query_embedding = np.array(query_embedding, dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "    # Safety checks\n",
    "    if index.ntotal == 0:\n",
    "        raise ValueError(\"FAISS index is empty. Add vectors before querying.\")\n",
    "\n",
    "    if query_embedding.shape[1] != index.d:\n",
    "        raise ValueError(f\"Embedding dimension mismatch: Query({query_embedding.shape[1]}) vs Index({index.d})\")\n",
    "\n",
    "    try:\n",
    "        distances, indices = index.search(query_embedding, k)\n",
    "        return distances, indices\n",
    "    except Exception as e:\n",
    "        print(\"Error in FAISS search:\", e)\n",
    "        return None, None\n",
    "\n",
    "def retrieve_answers(query, m_key):\n",
    "    \"\"\"Retrieve top-k similar documents for a query.\"\"\"\n",
    "    index, embedding_model = results[m_key]\n",
    "    distances, indices = similarity_search(query, embedding_model, index)\n",
    "\n",
    "    if indices is None:\n",
    "        return []\n",
    "\n",
    "    doc_texts = [df.iloc[i]['review_text'] for i in indices[0] if i >= 0]\n",
    "    return doc_texts\n",
    "\n",
    "# Test similarity search\n",
    "# sample_query = df.iloc[0]['review_text']\n",
    "# sample_index, sample_embedder = results['all-MiniLM-L6-v2']\n",
    "\n",
    "# distances, indices = similarity_search(sample_query, sample_embedder, sample_index)\n",
    "# if distances is not None:\n",
    "#     print(distances, indices)\n",
    "\n",
    "# Test answer query\n",
    "sample_query = 'What do people like about Spotify?'\n",
    "\n",
    "for m_key in models.keys():\n",
    "    answer = retrieve_answers(sample_query, m_key)\n",
    "    print(f\"Model: {m_key}\")\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the quality of documents retrieved and the time it has taken to process, we proceed with the first model: **paraphrase-MiniLM-L6-v2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choosen_model = \"paraphrase-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding LLM and a building retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for model: paraphrase-MiniLM-L6-v2\n",
      "embedder loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating documents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 457/457 [00:02<00:00, 174.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated docs\n",
      "one batch added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding batches:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 150/228 [3:42:52<1:55:53, 89.15s/it]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m docs = documents_from_df(df)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mgenerated docs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m vector_store = \u001b[43mbatch_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cache_file, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     37\u001b[39m     pickle.dump(vector_store, f)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mbatch_add\u001b[39m\u001b[34m(docs, embedder, batch_size)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(batch_size, \u001b[38;5;28mlen\u001b[39m(docs), batch_size), desc=\u001b[33m\"\u001b[39m\u001b[33mAdding batches\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     12\u001b[39m     batch = docs[i:i + batch_size]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     temp = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     vector_store.merge_from(temp)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m vector_store\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:844\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    841\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    842\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:1043\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> FAISS:\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     embeddings = \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1045\u001b[39m         texts,\n\u001b[32m   1046\u001b[39m         embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1050\u001b[39m         **kwargs,\n\u001b[32m   1051\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain/embeddings/cache.py:134\u001b[39m, in \u001b[36mCacheBackedEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    132\u001b[39m missing_texts = [texts[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m missing_indices]\n\u001b[32m    133\u001b[39m missing_vectors = \u001b[38;5;28mself\u001b[39m.underlying_embeddings.embed_documents(missing_texts)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdocument_embedding_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmissing_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_vectors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index, updated_vector \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(missing_indices, missing_vectors):\n\u001b[32m    138\u001b[39m     vectors[index] = updated_vector\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain/storage/encoder_backed.py:92\u001b[39m, in \u001b[36mEncoderBackedStore.mset\u001b[39m\u001b[34m(self, key_value_pairs)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Set the values for the given keys.\"\"\"\u001b[39;00m\n\u001b[32m     88\u001b[39m encoded_pairs = [\n\u001b[32m     89\u001b[39m     (\u001b[38;5;28mself\u001b[39m.key_encoder(key), \u001b[38;5;28mself\u001b[39m.value_serializer(value))\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m key_value_pairs\n\u001b[32m     91\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_pairs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain/storage/file_system.py:141\u001b[39m, in \u001b[36mLocalFileStore.mset\u001b[39m\u001b[34m(self, key_value_pairs)\u001b[39m\n\u001b[32m    139\u001b[39m full_path = \u001b[38;5;28mself\u001b[39m._get_full_path(key)\n\u001b[32m    140\u001b[39m \u001b[38;5;28mself\u001b[39m._mkdir_for_store(full_path.parent)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m \u001b[43mfull_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chmod_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     os.chmod(full_path, \u001b[38;5;28mself\u001b[39m.chmod_file)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/pathlib.py:1037\u001b[39m, in \u001b[36mPath.write_bytes\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1035\u001b[39m view = \u001b[38;5;28mmemoryview\u001b[39m(data)\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.open(mode=\u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mview\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "def batch_add(docs, embedder, batch_size=512):\n",
    "    batch = docs[0:batch_size]\n",
    "    vector_store = FAISS.from_documents(batch, embedder)\n",
    "    print('one batch added')\n",
    "    for i in tqdm(range(batch_size, len(docs), batch_size), desc=\"Adding batches\"):\n",
    "        batch = docs[i:i + batch_size]\n",
    "        temp = FAISS.from_documents(batch, embedder)\n",
    "        vector_store.merge_from(temp)\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "cache_dir = \"./cache\"\n",
    "cache_file = os.path.join(cache_dir, f\"{choosen_model}_embeddings.pickle\")\n",
    "embed_model = HuggingFaceEmbeddings(model_name=f\"{choosen_model}\", model_kwargs={\"device\": \"cuda\"})\n",
    "\n",
    "if os.path.exists(cache_file):\n",
    "    print(f'Loading embedding from cache: {cache_file}')\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        vector_store = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    print(f'Generating embeddings for model: {choosen_model}')\n",
    "    store = LocalFileStore(cache_dir)\n",
    "    embedder = CacheBackedEmbeddings.from_bytes_store(embed_model, store, namespace=f\"{choosen_model}\")\n",
    "    print(\"embedder loaded\")\n",
    "    docs = documents_from_df(df)\n",
    "    print(\"generated docs\")\n",
    "    vector_store = batch_add(docs, embedder)\n",
    "\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(vector_store, f)\n",
    "    print(f'Embeddings saved to cache: {cache_file}')\n",
    "\n",
    "# vector_store = results['all-MiniLM-L6-v2'][0]\n",
    "# documents = documents_from_df(df)\n",
    "# doc_dict = {i: doc for i, doc in enumerate(documents)}\n",
    "# doc_store = InMemoryDocstore({i: doc for i, doc in enumerate(documents)})\n",
    "# index_to_docstore_id = {i: i for i in range(len(documents))}\n",
    "\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "# llm_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# llm_model = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "llm_model = \"../model\"\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(llm_model)\n",
    "\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        llm_model,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float16       # use float16 to reduce memory footprint\n",
    "    )\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    return_full_text=True,\n",
    "    temperature=0.6,\n",
    "    max_new_tokens=250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "User question: {question}\n",
    "\n",
    "Answer the user's question using the following context.\n",
    "\n",
    "Follow these rules in order to answer the user's question:\n",
    "1) Your answer should be short (maximum 3 short sentences).\n",
    "2) Use the dataset information on Google Store reviews for Spotify to extract actionable insights.\n",
    "3) If you cannot recommend anything from the relevant information list, just say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Your answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template = prompt_template, input_variables=[\"question\", \"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    #verbose=True,\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18485/202821713.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa({\"question\": query, \"chat_history\": []})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User question: What do people like about Spotify?\n",
      "\n",
      "Answer the user's question using the following context.\n",
      "\n",
      "Follow these rules in order to answer the user's question:\n",
      "1) Your answer should be short (maximum 3 short sentences).\n",
      "2) Use the dataset information on Google Store reviews for Spotify to extract actionable insights.\n",
      "3) If you cannot recommend anything from the relevant information list, just say \"I don't know\".\n",
      "\n",
      "Context:\n",
      "i like every single thing about spotify\n",
      "\n",
      "incredible what is there to not like about spotify\n",
      "\n",
      "i love everything about spotify\n",
      "\n",
      "i love everything about spotify\n",
      "\n",
      "Your answer:\n",
      "I like every single thing about Spotify. I love everything about Spotify.\n",
      "\n",
      "I don't know.\n",
      "</think>\n",
      "\n",
      "I like every single thing about Spotify. I love everything about Spotify.\n",
      "\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "query = \"What do people like about Spotify?\"\n",
    "result = qa({\"question\": query, \"chat_history\": []})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def truncate_context(context):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "    max_tokens = 510  # leave space for special tokens and question tokens\n",
    "\n",
    "    tokens = tokenizer.encode(context, add_special_tokens=False)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        context = tokenizer.decode(tokens)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_text_from_query(query, context):\n",
    "\n",
    "    model = transformers.pipeline(\n",
    "        task=\"question-answering\",\n",
    "        model=\"distilbert-base-cased-distilled-squad\",\n",
    "        tokenizer=tokenizer,\n",
    "        # task=\"text-generation\",\n",
    "        return_full_text=True,\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=510\n",
    "    )\n",
    "\n",
    "    context = truncate_context(context)\n",
    "    # Pass both values to the QA pipeline\n",
    "    return model(question=query, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [3,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_text_from_query(query, context)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Then run your query\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43manswer_query\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat do people like about Spotify?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36manswer_query\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m      2\u001b[39m retrevial_result = retrieve_answers(query, \u001b[33m'\u001b[39m\u001b[33mall-MiniLM-L6-v2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m context = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(retrevial_result)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate_text_from_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mgenerate_text_from_query\u001b[39m\u001b[34m(query, context)\u001b[39m\n\u001b[32m     13\u001b[39m context = truncate_context(context)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Pass both values to the QA pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/pipelines/question_answering.py:398\u001b[39m, in \u001b[36mQuestionAnsweringPipeline.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    396\u001b[39m examples = \u001b[38;5;28mself\u001b[39m._args_parser(*args, **kwargs)\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(examples, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/pipelines/base.py:1360\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[32m   1359\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:269\u001b[39m, in \u001b[36mPipelinePackIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    266\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    271\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch.Tensor):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/pipelines/base.py:1275\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1274\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/pipelines/question_answering.py:525\u001b[39m, in \u001b[36mQuestionAnsweringPipeline._forward\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect.signature(model_forward).parameters.keys():\n\u001b[32m    524\u001b[39m     model_inputs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    527\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mstart\u001b[39m\u001b[33m\"\u001b[39m: output[\u001b[33m\"\u001b[39m\u001b[33mstart_logits\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mend\u001b[39m\u001b[33m\"\u001b[39m: output[\u001b[33m\"\u001b[39m\u001b[33mend_logits\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mexample\u001b[39m\u001b[33m\"\u001b[39m: example, **inputs}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:1099\u001b[39m, in \u001b[36mDistilBertForQuestionAnswering.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1087\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1088\u001b[39m \u001b[33;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1089\u001b[39m \u001b[33;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1095\u001b[39m \u001b[33;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1097\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1099\u001b[39m distilbert_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1108\u001b[39m hidden_states = distilbert_output[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, max_query_len, dim)\u001b[39;00m\n\u001b[32m   1110\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)  \u001b[38;5;66;03m# (bs, max_query_len, dim)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:793\u001b[39m, in \u001b[36mDistilBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    790\u001b[39m         attention_mask = torch.ones(input_shape, device=device)  \u001b[38;5;66;03m# (bs, seq_length)\u001b[39;00m\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m         attention_mask = \u001b[43m_prepare_4d_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformer(\n\u001b[32m    798\u001b[39m     x=embeddings,\n\u001b[32m    799\u001b[39m     attn_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    803\u001b[39m     return_dict=return_dict,\n\u001b[32m    804\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:448\u001b[39m, in \u001b[36m_prepare_4d_attention_mask_for_sdpa\u001b[39m\u001b[34m(mask, dtype, tgt_len)\u001b[39m\n\u001b[32m    445\u001b[39m is_tracing = torch.jit.is_tracing() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask, torch.fx.Proxy) \u001b[38;5;129;01mor\u001b[39;00m is_torchdynamo_compiling()\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# torch.jit.trace, symbolic_trace and torchdynamo with fullgraph=True are unable to capture data-dependent controlflows.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing \u001b[38;5;129;01mand\u001b[39;00m torch.all(mask == \u001b[32m1\u001b[39m):\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "def answer_query(query):\n",
    "    retrevial_result = retrieve_answers(query, f'{choosen_model}')\n",
    "    context = \" \".join(retrevial_result)\n",
    "    return generate_text_from_query(query, context)\n",
    "\n",
    "# Then run your query\n",
    "print(answer_query(\"What do people like about Spotify?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_query(\"What do users indiate they like about Spotify?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
