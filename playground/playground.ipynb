{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# from llama_index import (Document)\n",
    "\n",
    "# Load documents from the review dataset directory (adjust path as needed)\n",
    "csv_path = \"../Data/SPOTIFY_REVIEWS_DEDUP.csv\"\n",
    "df = pd.read_csv(csv_path, usecols=[\"review_text\",\"review_rating\",\"review_likes\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# from llama_index import (Document)\n",
    "\n",
    "# Load documents from the review dataset directory (adjust path as needed)\n",
    "csv_path = \"../Data/SPOTIFY_REVIEWS.csv\"\n",
    "df = pd.read_csv(csv_path, usecols=[\"review_text\",\"review_rating\",\"review_likes\"], nrows=1000000)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# remove rows with review text less than 5 tokens\n",
    "def remove_short_lines(df):\n",
    "    return df.loc[df[\"review_text\"].str.count(\" \") >= 4]\n",
    "\n",
    "def preprocess_text(df):\n",
    "    df.drop_duplicates(subset=[\"review_text\"], inplace=True)\n",
    "    df.dropna(subset=[\"review_text\"], inplace=True)\n",
    "    df[\"review_text\"] = (\n",
    "        df[\"review_text\"]\n",
    "        .str.replace(r'\\W+', ' ', regex=True)  # replace non-alphanumeric with space\n",
    "        .str.replace(r'\\s+', ' ', regex=True)  # collapse whitespace\n",
    "        .str.lower()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df = remove_short_lines(df)\n",
    "df = preprocess_text(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import importlib\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "# Load SymSpell dictionary (ensure you have \"frequency_dictionary_en.txt\")\n",
    "dictionary_path = importlib.resources.files(\"symspellpy\") / \"frequency_dictionary_en_82_765.txt\"\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2)\n",
    "sym_spell.load_dictionary(str(dictionary_path), term_index=0, count_index=1)\n",
    "\n",
    "def correct_spelling(text):\n",
    "    suggestions = sym_spell.lookup(text, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "    return suggestions[0].term if suggestions else text\n",
    "\n",
    "df[\"review_text\"] = df[\"review_text\"].apply(lambda x: correct_spelling(x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(df):\n",
    "    df['review_text'] = df['review_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "remove_stopwords(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import cudf\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as cuTfidfVectorizer\n",
    "from cuml.metrics.pairwise_distances import pairwise_distances\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import cupy as cp  # GPU-accelerated NumPy\n",
    "import cupyx.scipy.sparse as cpx  # Sparse CuPy operations\n",
    "from cuml.neighbors import NearestNeighbors  # RAPIDS GPU-optimized ANN\n",
    "\n",
    "def vectorized_deduplicate_dataframe(df, text_column=\"review_text\", threshold=0.8):\n",
    "    \"\"\"\n",
    "    Efficiently deduplicates a dataframe based on text similarity using TF-IDF and cosine similarity on GPU using RAPIDS cuML.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): Input DataFrame containing text data.\n",
    "    - text_column (str): Column name of text data.\n",
    "    - threshold (float): Similarity threshold (0 to 1), where higher means stricter deduplication.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Deduplicated DataFrame with all original columns preserved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute TF-IDF embeddings on GPU\n",
    "    vectorizer = cuTfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[text_column])  # Sparse matrix\n",
    "\n",
    "    print(\"Fitted and normalised matrix\")\n",
    "\n",
    "    # tfidf_matrix_gpu = cp.asarray(tfidf_matrix.toarray())  # Move data to GPU\n",
    "\n",
    "    # # Compute cosine similarity matrix on GPU\n",
    "    # similarity_matrix_gpu = pairwise_distances(tfidf_matrix_gpu, tfidf_matrix_gpu, metric=\"cosine\")\n",
    "    # print(\"Computed similarity matrix on GPU\")\n",
    "\n",
    "    # Convert to sparse CuPy matrix (Keep it sparse!)\n",
    "    tfidf_matrix_gpu = cpx.csr_matrix(tfidf_matrix)  \n",
    "\n",
    "    # Use RAPIDS NearestNeighbors (FAISS alternative)\n",
    "    nn = NearestNeighbors(n_neighbors=5, metric=\"cosine\", algorithm=\"brute\", output_type=\"numpy\")\n",
    "    nn.fit(tfidf_matrix_gpu)\n",
    "\n",
    "    print(\"Computed Nearest Neighbors on GPU\")\n",
    "\n",
    "    # Identify duplicates\n",
    "    unique_indexes = []\n",
    "    seen = set()\n",
    "\n",
    "    distances, indices = nn.kneighbors(tfidf_matrix_gpu, n_neighbors=5)\n",
    "\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        if i in seen:\n",
    "            continue\n",
    "        # Find similar reviews\n",
    "        # similar_indexes = pairwise_distances(similarity_matrix_gpu[i], similarity_matrix_gpu[i], metric=\"cosine\").toarray()\n",
    "        # similar_indexes = [idx for idx, val in enumerate(similar_indexes) if val > threshold]\n",
    "\n",
    "        similar_indexes = indices[i][distances[i] < threshold]        \n",
    "        seen.update(similar_indexes)  # Mark them as seen\n",
    "        unique_indexes.append(i)  # Keep only the first occurrence\n",
    "\n",
    "    # Return deduplicated DataFrame\n",
    "    return df.iloc[unique_indexes].reset_index(drop=True)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df_dedup1 = vectorized_deduplicate_dataframe(df)\n",
    "\n",
    "print(df_dedup1.shape)\n",
    "print(df.shape)\n",
    "\n",
    "df = df_dedup1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "data = {\n",
    "    \"review_text\": [\n",
    "        \"This product is amazing!\",  # Original\n",
    "        \"This product is really amazing!\",  # Slight variation\n",
    "        \"Worst product ever.\",  # Different\n",
    "        \"Absolutely terrible, do not buy.\",  # Different\n",
    "        \"Amazing product! Really good.\",  # Similar to first\n",
    "        \"I love this item, it's fantastic!\",  # Different\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_test = pd.DataFrame(data)\n",
    "\n",
    "# Run the deduplication function\n",
    "df_dedup = vectorized_deduplicate_dataframe(df_test, threshold=0.8)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nOriginal DataFrame:\")\n",
    "print(df_test)\n",
    "\n",
    "print(\"\\nDeduplicated DataFrame:\")\n",
    "print(df_dedup)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "df = df.dropna(subset=[\"review_text\"])  # or df[\"review_text\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# save this as a new csv\n",
    "df.to_csv(\"../Data/SPOTIFY_REVIEWS_DEDUP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#show 30 lines with length 5\n",
    "df[df[\"review_text\"].str.count(\" \") == 4].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from textblob import TextBlob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('punkt')\n",
    "\n",
    "df['num_tokens'] = df['review_text'].apply(lambda text: len(TextBlob(text).words))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['num_tokens'], kde=True, bins=50, color='blue')\n",
    "plt.xlabel('Number of tokens per review')\n",
    "plt.ylabel('Frequency')\n",
    "df.drop(columns=['num_tokens'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the reviews are quite short, we do not need to split them using text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# module_name = \"faiss\"\n",
    "# if module_name in sys.modules:\n",
    "#     del sys.modules[module_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import faiss\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "models = {\n",
    "    'all-MiniLM-L6-v2': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    'multi-qa-MiniLM-L6-dot-v1': 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1',\n",
    "    'paraphrase-MiniLM-L6-v2': 'sentence-transformers/paraphrase-MiniLM-L6-v2',\n",
    "}\n",
    "\n",
    "def select_model(m_key):\n",
    "    if m_key not in models:\n",
    "        raise ValueError(f'Model key is not found')\n",
    "    return SentenceTransformer(m_key, device=device)\n",
    "\n",
    "# import faiss\n",
    "# import torch\n",
    "\n",
    "# # Assume you have an index (e.g. a flat index on CPU)\n",
    "# cpu_index = faiss.IndexFlatL2(128)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"Using FAISS GPU\")\n",
    "#     # Create GPU resources and transfer the index to GPU 0\n",
    "#     res = faiss.StandardGpuResources()\n",
    "#     gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "#     index = gpu_index\n",
    "# else:\n",
    "#     print(\"Using FAISS CPU\")\n",
    "#     index = cpu_index\n",
    "\n",
    "# # Now use `index` for your similarity search\n",
    "\n",
    "# def encode_in_batches(texts, embedding_model, batch_size=512):\n",
    "#     sample_emb = embedding_model.encode([df.iloc[0]['review_text']], device=device)\n",
    "#     emb_dim = len(sample_emb[0])\n",
    "\n",
    "#     index = faiss.IndexFlatL2(emb_dim)\n",
    "\n",
    "#     for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding batches\"):\n",
    "#         batch_texts = texts[i:i + batch_size]\n",
    "#         emb = embedding_model.encode(batch_texts, device=device)\n",
    "#         index.add(emb)\n",
    "\n",
    "#     print(\"Compressing index\")\n",
    "#     m = 64  # Number of sub-vectors\n",
    "#     clusters = 100  # Number of clusters\n",
    "#     index = faiss.IndexIVFPQ(index, emb_dim, clusters, m, 8)  # IVF with PQ (100 clusters, 8-bit codes)\n",
    "\n",
    "#     return index\n",
    "\n",
    "def encode_in_batches(texts, embedding_model, batch_size=512):\n",
    "    # Get embedding dimension\n",
    "    sample_emb = embedding_model.encode([texts[0]], device=device)\n",
    "    emb_dim = sample_emb.shape[1] if len(sample_emb.shape) > 1 else sample_emb.shape[0]\n",
    "\n",
    "    num_vectors = len(texts)\n",
    "    \n",
    "    # Dynamically adjust number of clusters\n",
    "    clusters = min(256, max(10, num_vectors // 40))  \n",
    "\n",
    "    if num_vectors < 9984:\n",
    "        print(f\"Dataset has only {num_vectors} vectors. Using IndexFlatL2 instead.\")\n",
    "        index = faiss.IndexFlatL2(emb_dim)  # Flat index (no training needed)\n",
    "    else:\n",
    "        print(f\"Using {clusters} clusters for IVF-PQ.\")\n",
    "\n",
    "        quantizer = faiss.IndexFlatL2(emb_dim)\n",
    "        index = faiss.IndexIVFPQ(quantizer, emb_dim, clusters, 64, 8)\n",
    "\n",
    "        # Collect training data\n",
    "        training_data = []\n",
    "        for i in tqdm(range(0, num_vectors, batch_size), desc=\"Encoding batches (Training)\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            emb = embedding_model.encode(batch_texts, device=device).astype(np.float32)\n",
    "            training_data.append(emb)\n",
    "            if len(training_data) * batch_size >= clusters * 40:\n",
    "                break\n",
    "\n",
    "        training_data = np.vstack(training_data)\n",
    "        print(f\"Training IVF-PQ with {training_data.shape[0]} vectors...\")\n",
    "        index.train(training_data)\n",
    "\n",
    "    # Add vectors\n",
    "    for i in tqdm(range(0, num_vectors, batch_size), desc=\"Encoding batches (Indexing)\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        emb = embedding_model.encode(batch_texts, device=device).astype(np.float32)\n",
    "        index.add(emb)\n",
    "\n",
    "    print(\"Index built successfully!\")\n",
    "    return index\n",
    "\n",
    "def gen_emb_store(df, m_key, cache_dir = './cache'):\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "\n",
    "    # cache_file = os.path.join(cache_dir, f'{m_key}-embeddings.pickle')\n",
    "    cache_file = os.path.join(cache_dir, f\"{m_key}-index.idx\")\n",
    "\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f'Loading embedding from cache: {cache_file}')\n",
    "        # with open(cache_file, 'rb') as f:\n",
    "        #     vector_store = pickle.load(f)\n",
    "        # embed_model = select_model(m_key)\n",
    "\n",
    "        #load the faiss index\n",
    "        vector_store = faiss.read_index(f\"{cache_dir}/{m_key}-index.idx\")\n",
    "        embed_model = select_model(m_key)\n",
    "\n",
    "    else:\n",
    "        print(f'Generating embeddings for model: {m_key}')\n",
    "        embed_model = select_model(m_key)\n",
    "        texts = df['review_text'].tolist()\n",
    "        vector_store = encode_in_batches(texts, embed_model)\n",
    "\n",
    "        faiss.write_index(vector_store, f\"{cache_dir}/{m_key}-index.idx\")\n",
    "\n",
    "    return vector_store, embed_model\n",
    "\n",
    "def compare_models(df, model_keys, cache_dir='./cache'):\n",
    "    results = {}\n",
    "    for m_key in model_keys:\n",
    "        print(f'Processing model: {m_key}')\n",
    "        vector_store, embedder = gen_emb_store(df, m_key, cache_dir)\n",
    "        if vector_store is None:\n",
    "            raise ValueError(f\"Vector store creation failure for {m_key}\")\n",
    "\n",
    "        results[m_key] = (vector_store, embedder)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from tqdm import tqdm\n",
    "\n",
    "def documents_from_df(df, text_field=\"review_text\", batch_size=256):\n",
    "    \"\"\"\n",
    "    Convert a DataFrame into a list of Document objects.\n",
    "    The text_field parameter indicates which column holds the text.\n",
    "    \"\"\"\n",
    "    # Convert each row to a dictionary and use the selected text_field for page_content.\n",
    "    records = df.to_dict(orient=\"records\")\n",
    "    #Documentise the df in BATCHES\n",
    "    docs = []\n",
    "    for idx in tqdm(range(0, len(records), batch_size), desc=\"creating documents\"):\n",
    "        batch_records = records[idx: idx + batch_size]\n",
    "        docs.extend([Document(page_content=rec[text_field], metadata=rec) for rec in batch_records])\n",
    "\n",
    "    return docs\n",
    "\n",
    "    # return [Document(page_content=rec[text_field], metadata=rec) for rec in records]\n",
    "\n",
    "        \n",
    "    # store = LocalFileStore(cache_dir)\n",
    "    # embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    #     embed_model, store, namespace = m_key\n",
    "    # )\n",
    "    # vector_store = FAISS.from_documents(docs, embedder)\n",
    "\n",
    "    # with open(cache_file, 'wb') as f:\n",
    "    #     pickle.dump(vector_store, f)\n",
    "    # print(f'Embeddings saved to cache: {cache_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "if False:\n",
    "    sample_docs = df[:5000]\n",
    "\n",
    "    def time_for_m(m_key, docs):\n",
    "        start_time = time.time()\n",
    "        gen_emb_store(docs, m_key)\n",
    "        end_time = time.time()\n",
    "        return end_time - start_time\n",
    "\n",
    "    num_docs = len(df)\n",
    "    sample_size = len(sample_docs)\n",
    "\n",
    "    for m in models:\n",
    "        time_taken = time_for_m(m, sample_docs)\n",
    "        print(f\"Time taken for {m}: {time_taken} seconds\")\n",
    "        est_time_per_m = (time_taken / sample_size) * num_docs\n",
    "        print(f\"Estimated time for {m} on full dataset: {est_time_per_m} seconds\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "results = compare_models(df, models.keys(), cache_dir='./cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def similarity_search(query, embedding_model, index, k=20):\n",
    "    \"\"\"Perform similarity search using FAISS.\"\"\"\n",
    "    \n",
    "    # Encode query\n",
    "    query_embedding = embedding_model.encode([query], device=device)\n",
    "    \n",
    "    # Ensure correct dtype and shape\n",
    "    query_embedding = np.array(query_embedding, dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "    # Safety checks\n",
    "    if index.ntotal == 0:\n",
    "        raise ValueError(\"FAISS index is empty. Add vectors before querying.\")\n",
    "\n",
    "    if query_embedding.shape[1] != index.d:\n",
    "        raise ValueError(f\"Embedding dimension mismatch: Query({query_embedding.shape[1]}) vs Index({index.d})\")\n",
    "\n",
    "    try:\n",
    "        distances, indices = index.search(query_embedding, k)\n",
    "        return distances, indices\n",
    "    except Exception as e:\n",
    "        print(\"Error in FAISS search:\", e)\n",
    "        return None, None\n",
    "\n",
    "def retrieve_answers(query, index, embedding_model, k=20):\n",
    "    \"\"\"Retrieve top-k similar documents for a query.\"\"\"\n",
    "    distances, indices = similarity_search(query, embedding_model, index, k)\n",
    "\n",
    "    if indices is None:\n",
    "        return []\n",
    "\n",
    "    doc_texts = [df.iloc[i]['review_text'] for i in indices[0] if i >= 0]\n",
    "    return doc_texts\n",
    "\n",
    "# Test similarity search\n",
    "# sample_query = df.iloc[0]['review_text']\n",
    "# sample_index, sample_embedder = results['all-MiniLM-L6-v2']\n",
    "\n",
    "# distances, indices = similarity_search(sample_query, sample_embedder, sample_index)\n",
    "# if distances is not None:\n",
    "#     print(distances, indices)\n",
    "\n",
    "# Test answer query\n",
    "# sample_query = 'What do people like about Spotify?'\n",
    "\n",
    "# for m_key in models.keys():\n",
    "#     answer = retrieve_answers(sample_query, m_key)\n",
    "#     print(f\"Model: {m_key}\")\n",
    "#     print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the quality of documents retrieved and the time it has taken to process, we proceed with the first model: **paraphrase-MiniLM-L6-v2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_model = \"paraphrase-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the retrevial of chosen_model\n",
    "sample_query = 'What do people like about Spotify?'\n",
    "\n",
    "\n",
    "index,model = gen_emb_store(df, chosen_model)\n",
    "answer = retrieve_answers(sample_query, index, model)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding LLM and a building retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache file: .\\cache\\paraphrase-MiniLM-L6-v2-embeddings.pickle\n",
      "Loading embedding from cache: .\\cache\\paraphrase-MiniLM-L6-v2-embeddings.pickle\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "def batch_add(docs, embedder, batch_size=512):\n",
    "    batch = docs[0:batch_size]\n",
    "    vector_store = FAISS.from_documents(batch, embedder)\n",
    "    print('one batch added')\n",
    "    for i in tqdm(range(batch_size, len(docs), batch_size), desc=\"Adding batches\"):\n",
    "        batch = docs[i:i + batch_size]\n",
    "        temp = FAISS.from_documents(batch, embedder)\n",
    "        vector_store.merge_from(temp)\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "cache_dir = \"cache\"\n",
    "cache_file = os.path.join(\".\", cache_dir, f\"{chosen_model}-embeddings.pickle\")\n",
    "print(f\"Cache file: {cache_file}\")\n",
    "embed_model = HuggingFaceEmbeddings(model_name=f\"{chosen_model}\", model_kwargs={\"device\": \"cuda\"})\n",
    "\n",
    "if os.path.exists(cache_file):\n",
    "    print(f'Loading embedding from cache: {cache_file}')\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        vector_store = pickle.load(f)\n",
    "        \n",
    "else:\n",
    "    print(f'Generating embeddings for model: {chosen_model}')\n",
    "    store = LocalFileStore(cache_dir)\n",
    "    embedder = CacheBackedEmbeddings.from_bytes_store(embed_model, store, namespace=f\"{chosen_model}\")\n",
    "    print(\"embedder loaded\")\n",
    "    docs = documents_from_df(df)\n",
    "    print(\"generated docs\")\n",
    "    vector_store = batch_add(docs, embedder)\n",
    "\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(vector_store, f)\n",
    "    print(f'Embeddings saved to cache: {cache_file}')\n",
    "\n",
    "# vector_store = results['all-MiniLM-L6-v2'][0]\n",
    "# documents = documents_from_df(df)\n",
    "# doc_dict = {i: doc for i, doc in enumerate(documents)}\n",
    "# doc_store = InMemoryDocstore({i: doc for i, doc in enumerate(documents)})\n",
    "# index_to_docstore_id = {i: i for i in range(len(documents))}\n",
    "pathlib.PosixPath = temp\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# torch.backends.cuda.enable_flash_sdp(True)\n",
    "# torch.backends.cuda.enable_math_sdp(False)\n",
    "# torch.backends.cuda.enable_mem_efficient_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# llm_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# llm_model = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "llm_model = \"../model\"\n",
    "\n",
    "# hf_token = os.getenv('HUGGINGFACE_HUB_TOKEN') \n",
    "# login(token=hf_token)\n",
    "\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(llm_model)\n",
    "\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        llm_model,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float16       # use float16 to reduce memory footprint\n",
    "    )\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    return_full_text=True,\n",
    "    temperature=0.5,\n",
    "    max_new_tokens=250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "User question: {question}\n",
    "\n",
    "Answer the user's question using the following context.\n",
    "\n",
    "Follow these rules in order to answer the user's question:\n",
    "1) Your answer should be short (maximum 3 short coherent sentences).\n",
    "2) Use the dataset information on Google Store reviews for Spotify to extract actionable insights.\n",
    "3) Your answer should be a coherent question answering the question with the given context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Your answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template = prompt_template, input_variables=[\"question\", \"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    #verbose=True,\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanw\\AppData\\Local\\Temp\\ipykernel_20884\\202821713.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa({\"question\": query, \"chat_history\": []})\n",
      "c:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User question: What do people like about Spotify?\n",
      "\n",
      "Answer the user's question using the following context.\n",
      "\n",
      "Follow these rules in order to answer the user's question:\n",
      "1) Your answer should be short (maximum 3 short coherent sentences).\n",
      "2) Use the dataset information on Google Store reviews for Spotify to extract actionable insights.\n",
      "3) Your answer should be a coherent question answering the question with the given context.\n",
      "\n",
      "Context:\n",
      "really like enjoy spotify\n",
      "\n",
      "really like spotify cool\n",
      "\n",
      "really like spotify complaints\n",
      "\n",
      "really enjoy spotify best\n",
      "\n",
      "Your answer:\n",
      "I like the music quality, it's better than what I expected, and the layout is also nice.\n",
      "\n",
      "I enjoy the music and the layout is good.\n",
      "\n",
      "I really like the music quality and the layout is nice.\n",
      "\n",
      "I really like the music and the layout is good.\n",
      "\n",
      "I really like the music and the layout is nice.\n",
      "\n",
      "I really like the music and the layout is good.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is better than what I expected.\n",
      "\n",
      "I really like the layout and the music is\n"
     ]
    }
   ],
   "source": [
    "query = \"What do people like about Spotify?\"\n",
    "result = qa({\"question\": query, \"chat_history\": []})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def truncate_context(context):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "    max_tokens = 510  # leave space for special tokens and question tokens\n",
    "\n",
    "    tokens = tokenizer.encode(context, add_special_tokens=False)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        context = tokenizer.decode(tokens)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_text_from_query(query, context):\n",
    "\n",
    "    model = transformers.pipeline(\n",
    "        task=\"question-answering\",\n",
    "        model=\"distilbert-base-cased-distilled-squad\",\n",
    "        tokenizer=tokenizer,\n",
    "        # task=\"text-generation\",\n",
    "        return_full_text=True,\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=510\n",
    "    )\n",
    "\n",
    "    context = truncate_context(context)\n",
    "    # Pass both values to the QA pipeline\n",
    "    return model(question=query, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding from cache: ./cache\\paraphrase-MiniLM-L6-v2-index.idx\n"
     ]
    }
   ],
   "source": [
    "index, model = gen_emb_store(df, chosen_model, cache_dir='./cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_text_from_query(query, context)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Then run your query\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43manswer_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat do people like about Spotify?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m, in \u001b[0;36manswer_query\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      2\u001b[0m retrevial_result \u001b[38;5;241m=\u001b[39m retrieve_answers(query, index, model)\n\u001b[0;32m      3\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(retrevial_result)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate_text_from_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 15\u001b[0m, in \u001b[0;36mgenerate_text_from_query\u001b[1;34m(query, context)\u001b[0m\n\u001b[0;32m     13\u001b[0m context \u001b[38;5;241m=\u001b[39m truncate_context(context)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Pass both values to the QA pipeline\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:398\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args_parser(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:1293\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[1;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:1208\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1207\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1208\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1209\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:525\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline._forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    524\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 525\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m\"\u001b[39m: example, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs}\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:1100\u001b[0m, in \u001b[0;36mDistilBertForQuestionAnswering.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1100\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1109\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, max_query_len, dim)\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)  \u001b[38;5;66;03m# (bs, max_query_len, dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:794\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    791\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(input_shape, device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# (bs, seq_length)\u001b[39;00m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[1;32m--> 794\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_4d_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m    799\u001b[0m     x\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m    800\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    804\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    805\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\seanw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:444\u001b[0m, in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[1;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[0;32m    441\u001b[0m is_tracing \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mProxy) \u001b[38;5;129;01mor\u001b[39;00m is_torchdynamo_compiling()\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# torch.jit.trace, symbolic_trace and torchdynamo with fullgraph=True are unable to capture data-dependent controlflows.\u001b[39;00m\n\u001b[1;32m--> 444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def answer_query(query):\n",
    "    retrevial_result = retrieve_answers(query, index, model)\n",
    "    context = \" \".join(retrevial_result)\n",
    "    return generate_text_from_query(query, context)\n",
    "\n",
    "# Then run your query\n",
    "print(answer_query(\"What do people like about Spotify?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_query(\"What do users indiate they like about Spotify?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
