{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sentence_transformers import util\n",
    "\n",
    "# # def similarity_search(query, embedding_model, index, k=10):\n",
    "# #     query_embedding = embedding_model.encode([query], device=device)\n",
    "# #     distances, indices = index.search(query_embedding, k)\n",
    "# #     return distances, indices\n",
    "\n",
    "# def similarity_search(query, embedding_model, index, k=10):\n",
    "#     query_embedding = embedding_model.encode([query], device=device)\n",
    "\n",
    "#     # Ensure correct dtype and shape\n",
    "#     query_embedding = np.array(query_embedding).astype('float32').reshape(1, -1)\n",
    "\n",
    "#     # Debugging prints\n",
    "#     print(\"Query embedding shape:\", query_embedding.shape)\n",
    "#     print(\"Index dimension:\", index.d)\n",
    "#     print(\"Index size:\", index.ntotal)\n",
    "\n",
    "#     distances, indices = index.search(query_embedding, k)\n",
    "#     return distances, indices\n",
    "\n",
    "# def retrieve_answers(query, m_key):\n",
    "#     index, embedding_model = results[m_key]\n",
    "#     distances, indices = similarity_search(query, embedding_model, index)\n",
    "#     doc_texts = [df.iloc[i]['review_text'] for i in indices[0]]\n",
    "#     return doc_texts\n",
    "\n",
    "# #test similarity search\n",
    "# sample_query = df.iloc[0]['review_text']\n",
    "# sample_index, sample_embedder = results['all-MiniLM-L6-v2']\n",
    "# distances, indices = similarity_search(sample_query, sample_embedder, sample_index)\n",
    "# print(distances, indices)\n",
    "\n",
    "# #test answer query\n",
    "# sample_query = 'What do people like about Spotify?'\n",
    "\n",
    "# for m_key in models.keys():\n",
    "#     answer = retrieve_answers(sample_query, m_key)\n",
    "#     print(f\"Model: {m_key}\")\n",
    "#     print(answer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 66688\n",
      "Processed rows 0 to 500000\n",
      "Processed rows 500000 to 1000000\n",
      "Processed rows 1000000 to 1500000\n",
      "Processed rows 1500000 to 1862328\n",
      "TF-IDF matrix is ready for GPU processing.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cudf\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as cuTfidfVectorizer\n",
    "import cupyx.scipy.sparse as cpx   # GPU sparse operations\n",
    "from cupyx.scipy.sparse import vstack   # To stack batches on GPU\n",
    "\n",
    "def batch_vectorize_tfidf(df, text_column=\"review_text\", sample_size = 500000, batch_size=500000):\n",
    "    \"\"\"\n",
    "    Fit a cuML TF-IDF vectorizer on a sample of the data and then transform the full dataset in batches.\n",
    "    \n",
    "    Args:\n",
    "      df (pd.DataFrame): Input DataFrame.\n",
    "      text_column (str): Column containing text.\n",
    "      sample_size (int): Number of rows used to fit the vocabulary.\n",
    "      batch_size (int): Batch size for transforming.\n",
    "    \n",
    "    Returns:\n",
    "      (vectorizer, full_sparse_matrix): The fitted vectorizer and combined sparse TF-IDF matrix.\n",
    "    \"\"\"\n",
    "    # Fit the vectorizer on a sample to learn the vocabulary.\n",
    "    sample_texts = df[text_column].iloc[:sample_size]\n",
    "    vectorizer = cuTfidfVectorizer(stop_words=\"english\")\n",
    "    vectorizer.fit(sample_texts)\n",
    "    print(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n",
    "    \n",
    "    # Process the full dataset in batches\n",
    "    batches = []\n",
    "    n = df.shape[0]\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = min(n, start + batch_size)\n",
    "        batch_texts = df[text_column].iloc[start:end]\n",
    "        batch_matrix = vectorizer.transform(batch_texts)\n",
    "        batches.append(batch_matrix)\n",
    "        print(f\"Processed rows {start} to {end}\")\n",
    "    \n",
    "    # Stack the batches together into one sparse matrix.\n",
    "    full_sparse_matrix = vstack(batches)\n",
    "    return vectorizer, full_sparse_matrix\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df is your DataFrame with 3.4M rows in \"review_text\"\n",
    "vectorizer, tfidf_matrix = batch_vectorize_tfidf(df, text_column=\"review_text\", sample_size=500000, batch_size=500000)\n",
    "\n",
    "# Now, convert the CPU-based sparse matrix to a CuPy sparse format for GPU operations.\n",
    "tfidf_matrix_gpu = cpx.csr_matrix(tfidf_matrix)\n",
    "print(\"TF-IDF matrix is ready for GPU processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as cuTfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import cupy as cp  # GPU-accelerated NumPy\n",
    "import cupyx.scipy.sparse as cpx  # Sparse CuPy operations\n",
    "from cuml.neighbors import NearestNeighbors  # RAPIDS GPU-optimized ANN\n",
    "\n",
    "def vectorized_deduplicate_dataframe(tfidf_matrix_gpu, threshold=0.8):\n",
    "\n",
    "    # Use RAPIDS NearestNeighbors (FAISS alternative)\n",
    "    nn = NearestNeighbors(n_neighbors=5, metric=\"cosine\", algorithm=\"brute\", output_type=\"cupy\")\n",
    "    nn.fit(tfidf_matrix_gpu)\n",
    "\n",
    "    print(\"Fitted\")\n",
    "\n",
    "    # Identify duplicates\n",
    "    unique_indexes = []\n",
    "    seen = set()\n",
    "\n",
    "    distances, indices = nn.kneighbors(tfidf_matrix_gpu, n_neighbors=5)\n",
    "\n",
    "    print(\"Computed Nearest Neighbors on GPU\")\n",
    "\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        if i in seen:\n",
    "            continue\n",
    "        # Find similar reviews\n",
    "        # similar_indexes = pairwise_distances(similarity_matrix_gpu[i], similarity_matrix_gpu[i], metric=\"cosine\").toarray()\n",
    "        # similar_indexes = [idx for idx, val in enumerate(similar_indexes) if val > threshold]\n",
    "\n",
    "        similar_indexes = indices[i][distances[i] < threshold]        \n",
    "        seen.update(similar_indexes)  # Mark them as seen\n",
    "        unique_indexes.append(i)  # Keep only the first occurrence\n",
    "\n",
    "    # Return deduplicated DataFrame\n",
    "    return df.iloc[unique_indexes].reset_index(drop=True)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df_dedup1 = vectorized_deduplicate_dataframe(tfidf_matrix_gpu)\n",
    "\n",
    "print(df_dedup1.shape)\n",
    "print(df.shape)\n",
    "\n",
    "df = df_dedup1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import cupy as cp\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "def vectorized_deduplicate_dataframe(df, text_column=\"review_text\", threshold=0.8):\n",
    "    \"\"\"\n",
    "    Efficiently deduplicates a dataframe based on text similarity using TF-IDF and cosine similarity on GPU.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): Input DataFrame containing text data.\n",
    "    - text_column (str): Column name of text data.\n",
    "    - threshold (float): Similarity threshold (0 to 1), where higher means stricter deduplication.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Deduplicated DataFrame with all original columns preserved.\n",
    "    \"\"\"\n",
    "    # Compute TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[text_column])\n",
    "\n",
    "    print(\"Fitted matrix\")\n",
    "\n",
    "    # Convert the sparse matrix to a dense matrix and transfer it to GPU\n",
    "    tfidf_matrix_gpu = cp.sparse.csr_matrix(tfidf_matrix).todense()\n",
    "    tfidf_matrix_gpu = cp.asarray(tfidf_matrix_gpu)\n",
    "\n",
    "    # Normalize the matrix to unit vectors (L2 normalization)\n",
    "    tfidf_matrix_gpu /= cp.linalg.norm(tfidf_matrix_gpu, axis=1)[:, cp.newaxis]\n",
    "    print(\"Normalized matrix\")\n",
    "\n",
    "    # Compute the cosine similarity matrix on GPU\n",
    "    similarity_matrix_gpu = cp.matmul(tfidf_matrix_gpu, tfidf_matrix_gpu.T)\n",
    "    print(\"Computed similarity matrix on GPU\")\n",
    "\n",
    "    # Identify duplicates\n",
    "    unique_indexes = []\n",
    "    seen = set()\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        if i in seen:\n",
    "            continue\n",
    "        # Find similar reviews\n",
    "        similar_indexes = cp.where(similarity_matrix_gpu[i] > threshold)[0].get()  # Transfer results to CPU\n",
    "        seen.update(similar_indexes)  # Mark them as seen\n",
    "        unique_indexes.append(i)  # Keep only the first occurrence\n",
    "\n",
    "    # Return the deduplicated DataFrame\n",
    "    return df.iloc[unique_indexes].reset_index(drop=True)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df_dedup1 = vectorized_deduplicate_dataframe(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "def vectorized_deduplicate_dataframe(df, text_column=\"review_text\", threshold=0.8):\n",
    "    \"\"\"\n",
    "    Efficiently deduplicates a dataframe based on text similarity using TF-IDF and cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): Input DataFrame containing text data.\n",
    "    - text_column (str): Column name of text data.\n",
    "    - threshold (float): Similarity threshold (0 to 1), where higher means stricter deduplication.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Deduplicated DataFrame with all original columns preserved.\n",
    "    \"\"\"\n",
    "    # Compute TF-IDF embeddings\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[text_column])\n",
    "    print(\"Fitted matrix\")\n",
    "\n",
    "    tfidf_matrix = normalize(tfidf_matrix, norm=\"l2\")\n",
    "    print(\"normalised\")\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    print(\"Computed similarity matrix\")\n",
    "\n",
    "    # Identify duplicates\n",
    "    unique_indexes = []\n",
    "    seen = set()\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        if i in seen:\n",
    "            continue\n",
    "        # Find similar reviews\n",
    "        similar_indexes = np.where(similarity_matrix[i] > threshold)[0]\n",
    "        seen.update(similar_indexes)  # Mark them as seen\n",
    "        unique_indexes.append(i)  # Keep only the first occurrence\n",
    "\n",
    "    # Return deduplicated DataFrame\n",
    "    return df.iloc[unique_indexes].reset_index(drop=True)\n",
    "\n",
    "df_dedup1 = vectorized_deduplicate_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_closest(vector_store, embedder, query, k=3):\n",
    "#     q_embed = embedder.embed_query(query)\n",
    "#     return vector_store.similarity_search_by_vector(q_embed, k=k) #, distances=[], labels=[])\n",
    "\n",
    "# query = 'What do people like about Spotify?'\n",
    "\n",
    "# comparisons = {}\n",
    "# for m_key, (vector_store, embedder) in results.items():\n",
    "#     close = get_closest(vector_store, embedder, query)\n",
    "#     comparisons[m_key] = close\n",
    "\n",
    "# for m_key, close in comparisons.items():\n",
    "#     print(f\"Model: {m_key}\")\n",
    "#     for n in close:\n",
    "#         print(n)\n",
    "#     print(\"---------------------------------------- \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import faiss\n",
    "# import torch\n",
    "\n",
    "# # Assume you have an index (e.g. a flat index on CPU)\n",
    "# cpu_index = faiss.IndexFlatL2(128)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"Using FAISS GPU\")\n",
    "#     # Create GPU resources and transfer the index to GPU 0\n",
    "#     res = faiss.StandardGpuResources()\n",
    "#     gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "#     index = gpu_index\n",
    "# else:\n",
    "#     print(\"Using FAISS CPU\")\n",
    "#     index = cpu_index\n",
    "\n",
    "# # Now use `index` for your similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deduplicating reviews: 100%|██████████| 3004/3004 [00:03<00:00, 828.49it/s] \n"
     ]
    }
   ],
   "source": [
    "# from rapidfuzz import process, fuzz\n",
    "\n",
    "# # Function to remove duplicate reviews while keeping all columns\n",
    "# def deduplicate_dataframe(df, text_column=\"review_text\", threshold=80):\n",
    "#     seen = {}  # Dictionary to track unique reviews with indexes\n",
    "#     unique_indexes = []\n",
    "\n",
    "#     for index, review in tqdm(df[text_column].items(), total=len(df), desc=\"Deduplicating reviews\"):\n",
    "#         # Find best match from seen reviews\n",
    "#         match = process.extractOne(review, seen.keys(), scorer=fuzz.ratio)\n",
    "        \n",
    "#         # If no close match or similarity is below threshold, keep review\n",
    "#         if not match or match[1] < threshold:\n",
    "#             seen[review] = index\n",
    "#             unique_indexes.append(index)\n",
    "\n",
    "#     # Return deduplicated DataFrame\n",
    "#     return df.loc[unique_indexes].reset_index(drop=True)\n",
    "\n",
    "# # Apply deduplication while preserving all columns\n",
    "# df_dedup = deduplicate_dataframe(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (3004, 3)\n",
      "Deduplicated DataFrame shape: (2951, 3)\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Original DataFrame shape: {df.shape}\")\n",
    "# print(f\"Deduplicated DataFrame shape: {df_dedup.shape}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
